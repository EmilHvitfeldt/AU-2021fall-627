{
  "articles": [
    {
      "path": "assignment-01.html",
      "title": "Assignment 1",
      "author": [],
      "contents": "\nExercise 1 (5 points)\nFor each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.\nThe sample size \\(n\\) is extremely large, and the number of predictors \\(p\\) is small.\nThe number of predictors \\(p\\) is extremely large, and the number of observations \\(n\\) is small.\nThe relationship between the predictors and response is highly non-linear.\nThe variance of the error terms, is extremely high.\nExercise 2 (5 points)\nDescribe the difference between a parametric and non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a noon-parametric approach)? What are its disadvantages?\nExercise 3 (5 points)\nCarefully explain the the difference between the KNN classifier and KNN regression methods. Name a downside when using this model on very large data.\nExercise 4 (5 points)\nSuppose we have a data set with five predictors, \\(X1 =\\) GPA, \\(X2 =\\) extracurricular activities (EA), \\(X3 =\\) Gender (1 for Female and 0 for Male), \\(X4 =\\) Interaction between GPA and EA, and \\(X5 =\\) Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get \\(\\beta_0 = 50\\), \\(\\beta_1 = 20\\), \\(\\beta_2 = 0.07\\), \\(\\beta_3 = 35\\), \\(\\beta_4 = 0.01\\), \\(\\beta_5 = - 10\\).\nWhich answer is correct, and why?\nFor a fixed value of EA and GPA, males earn more on average than females.\nFor a fixed value of EA and GPA, females earn more on average than males.\nFor a fixed value of EA and GPA, males earn more on average than females provided that the GPA is high enough.\nFor a fixed value of EA and GPA, females earn more on average than males provided that the GPA is high enough.\n\nPredict the salary of a female with EA of 110 and a GPA of 4.0.\nTrue or false: Since the coefficient for the GPA/EA interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\nExercise 5 (10 points)\nThis question should be answered using the biomass data set.\n\n\nlibrary(tidymodels)\ndata(\"biomass\")\n\n\n\nFit a multiple regression model to predict HHV using carbon, hydrogen and oxygen.\nProvide an interpretation of each coefficient in the model. Be careful, note the values Cruise is able to take.\nWrite out the model in equation form.\nFor which the predictors can you reject the null hypothesis \\(H_0: \\beta_j = 0\\)?\nOn the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.\nHow well do the models in (a) and (e) fit the data? How big was the effect of removing the predictor?\n\n\n\n",
      "last_modified": "2021-10-17T14:33:13-07:00"
    },
    {
      "path": "assignment-02.html",
      "title": "Assignment 2",
      "author": [],
      "contents": "\nExercise 1 (10 points)\nSuppose we collect data for a group of students in a statistics class with variables \\(X_1\\) = hours studied, \\(X_2\\) = undergrad GPA, and \\(Y\\) = receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat{\\beta}_0=-6\\), \\(\\hat{\\beta}_1=0.05\\), \\(\\hat{\\beta}_2=1\\).\nEstimate the probability that a student who studies for 40 hours and has an undergrad GPA of \\(3.5\\) gets an A in the class.\nHow many hours would that student in part (a) need to study to have a 50% chance of getting an A in the class?\nExercise 2 (5 points)\nSuppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First, we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next, we use 1-nearest neighbors (i.e. \\(K = 1\\)) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?\nExercise 3 (15 points)\nIn this exercise, we will explore a data set about cars called auto which you can find here.\nThe data set contains 1 factor variable and 6 numeric variables. The factor variable mpg has two levels high and low indicating whether the car has a high or low miles per gallon. We will in this exercise investigate if we are able to use a logistic regression classifier to predict if a car has high or low mpg from the other variables.\nRead in the data and create a test-train rsplit object of auto using initial_split(). Use default arguments for initial_split().\nCreate the training and testing data set with training() and testing() respectively.\nFit a logistic regression model using logistic_reg(). Use all the 6 numeric variables as predictors (a formula shorthand is to write mpg ~ . where . means everything. Remember to fit the model only using the training data set.\nInspect the model with summary() and tidy(). Which of the variables are significant?\nPredict values for the training data set and save them as training_pred.\nUse the following code to calculate the training accuracy\n\n\nbind_cols(\n  training_pred,\n  auto_training\n) %>%\n  accuracy(truth = mpg, estimate = .pred_class)\n\n\n\n(auto_training should be renamed to match your training data set if needed.)\nPredict values for the testing data set and use the above code to calculate the testing accuracy. Compare.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:13-07:00"
    },
    {
      "path": "assignment-03.html",
      "title": "Assignment 3",
      "author": [],
      "contents": "\nExercise 1 (10 points)\nYou will in this exercise examine the differences between LDA and QDA.\nIf the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\nIf the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\nIn general, as the sample size \\(n\\) increases, do we expect the test prediction accuracy or QDA relative to LDA to improve, decline, or be unchanged? Why?\nTrue or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\nExecise 3 (20 points)\nThis exercise should be answered using the Weekly data set, which is part of the LSLR package. If you don’t have it installed already you can install it with\n\n\ninstall.packages(\"ISLR\")\n\n\n\nTo load the data set run the following code\n\n\nlibrary(ISLR)\ndata(\"Weekly\")\n\n\n\nThis data is similar in nature to the Smarket data from chapter 4’s lab, it contains 1089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.\nProduce some numerical and graphical summaries of the data. Does there appear to be any patterns?\nUse the whole data set to perform a logistic regression (with logistic_reg()) with Direction as the response and the five lag variables plus Volume as predictors. Use the summary() function to print the results. Do any of the predictors appear to be statistically significant? if so, which ones?\nUse conf_int() and accuracy() from yardstick package to calculate the confusion matrix and the accuracy (overall fraction of correct predictions). Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\nSplit the data into a training and testing data set using the following code\n\n\nweekly_training <- Weekly %>% filter(Year <= 2008)\n weekly_testing <- Weekly %>% filter(Year > 2008)\n \n\n\nNow fit the logistic regression model using the training data, with Lag2 as the only predictor. Compute the confusion matrix and accuracy metric using the testing data set.\nRepeat (e) using LDA.\nRepeat (e) using QDA.\nRepeat (e) using KNN with K = 1.\nWhich of these methods appear to provide the best results on the data?\n(Optional) Experiment with different combinations of predictors for each of the methods. Report the variables, methods, and associated confusion matrix that appears to provide the best results on the held-out data. Note that you can also experiment with different values of K in KNN. (This kind of running many many models and testing on the testing data set many times is not good practice. We will look at ways in later weeks on how we can properly explore multiple models.)\n\n\n\n",
      "last_modified": "2021-10-17T14:33:14-07:00"
    },
    {
      "path": "assignment-04.html",
      "title": "Assignment 4",
      "author": [],
      "contents": "\nExercise 1 (7.5 points)\nReview of k-fold cross-validation.\nExplain how k-fold cross-validation is implemented.\nWhat are the advantages and disadvantages of k-fold cross-validation relative to\nThe validation set approach\nLOOCV\n\nExercise 2 (7.5 points)\nDenote whether the following statements are true or false. Explain your reasoning.\nWhen \\(k = n\\) the cross-validation estimator is approximately unbiased for the true prediction error.\nWhen \\(k = n\\) the cross-validation estimator will always have a low variance.\nStatistical transformations on the predictors, such as scaling and centering, must be done inside each fold.\nExercise 3 (15 points)\nThis exercise should be answered using the Weekly data set, which is part of the LSLR package. If you don’t have it installed already you can install it with\n\n\ninstall.packages(\"ISLR\")\n\n\n\nTo load the data set run the following code\n\n\nlibrary(ISLR)\ndata(\"Weekly\")\n\n\n\nCreate a test and training set using initial_split(). Split proportion is up to you. Remember to set a seed!\nCreate a logistic regression specification using logistic_reg(). Set the engine to glm.\nCreate a 5-fold cross-validation object using the training data, and fit the resampled folds with fit_resamples() and Direction as the response and the five lag variables plus Volume as predictors. Remember to set a seed before creating k-fold object.\nCollect the performance metrics using collect_metrics(). Interpret.\nFit the model on the whole training data set. Calculate the accuracy on the test set. How does this result compare to results in d. Interpret.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:15-07:00"
    },
    {
      "path": "assignment-05.html",
      "title": "Assignment 5",
      "author": [],
      "contents": "\nExercise 1 (10 points)\nWe will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of \\(n\\) observations.\nWhat is the probability that the first bootstrap observation is not the \\(j\\)th observation from the original sample? Justify your answer.\nWhat is the probability that the second bootstrap observation is not the \\(j\\)th observation from the original sample? Justify your answer.\nArgue that the probability that the \\(j\\)th observation is not in the bootstrap sample is \\((1-1/n)^n\\).\nWhen \\(n = 5\\) what is the probability that the \\(j\\)th observation is in the bootstrap sample?\nWhen \\(n = 100\\) what is the probability that the \\(j\\)th observation is in the bootstrap sample?\nWhen \\(n = 10,000\\) what is the probability that the \\(j\\)th observation is in the bootstrap sample?\nCreate a plot that displays, for each integer value of \\(n\\) from 1 to 100,000 the probability that the \\(j\\)th observation is in the bootstrap sample. Comment on what you observe.\nWe will now investigate numerically that a bootstrap sample of size \\(n = 100\\) contains the \\(j\\)th observation. Here \\(j = 4\\). We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample.\n\n\nset.seed(  ) # set a seed here\n\nstore <- integer(10000)\n\nfor (i in seq_along(store)) {\n  store[i] <- sum(sample(seq_len(100), replace = TRUE) == 4) > 0\n}\n\nmean(store)\n\n\n\nComment on the results obtained.\nExercise 2 (10 points)\nSuppose that we use some statistical learning method to make a prediction for the response \\(Y\\) for a particular value of the predictor \\(X\\).\nCarefully describe how we might estimate the standard deviation of our prediction.\nIs this procedure depends on what statistical learning method we are using?\nExercise 3 (10 points)\nThis exercise should be answered using the Default data set, which is part of the LSLR package. If you don’t have it installed already you can install it with\n\n\ninstall.packages(\"ISLR\")\n\n\n\nTo load the data set run the following code\n\n\nlibrary(ISLR)\ndata(\"Default\")\n\n\n\nUse the parsnip package to fit a logistic regression on the default data set. default is the response and income and balance are the predictors. Then use summary() on the fitted model to determine the estimated standard errors for the coefficients associated with income and balance. Comment on the estimated standard errors.\nUse the bootstraps() function from the rsample package to generate 25 bootstraps of Default.\nRun the following code. Change boots to the name of the bootstrapping object created in the previous question. This will take a minute or two to run. Comment on\n\n\n# This function takes a `bootstrapped` split object, and fits a logistic model\nfit_lr_on_bootstrap <- function(split) {\n  logistic_reg() %>%\n    set_engine(\"glm\") %>%\n    set_mode(\"classification\") %>%\n    fit(default ~ income + balance, analysis(split))\n}\n\n# This code uses `map()` to run the model inside each split. Then it used\n# `tidy()` to extract the model estimates the parameter\nboot_models <-\n  boots %>% \n  mutate(model = map(splits, fit_lr_on_bootstrap),\n         coef_info = map(model, tidy))\n\n# This code extract the estimates for each model that was fit\nboot_coefs <-\n  boot_models %>% \n  unnest(coef_info)\n\n# This code calculates the standard deviation of the estimate\nsd_estimates <- boot_coefs %>%\n  group_by(term) %>%\n  summarise(std.error_est = sd(estimate))\nsd_estimates\n\n\n\nComment on the estimated standard errors obtained using the summary() function on the first model and the estimated standard errors you found using the above code.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:15-07:00"
    },
    {
      "path": "assignment-06.html",
      "title": "Assignment 6",
      "author": [],
      "contents": "\nExercise 1\nExplain the assumptions we are making when performing Principle Component Analysis (PCA). What happens when these assumptions are violated?\nExercise 2\nAnswer the following questions regarding Principle Component Analysis.\nIs it important to standardize before applying PCA?\nShould one remove highly correlated variables before doing PCA?\nWhat will happen when eigenvalues are roughly equal?\nCan PCA be used to reduce the dimensionality of a highly nonlinear data set?\nExercise 3\nYou will in this exercise explore a data set using PCA. The data comes from the #tidytuesday project and is about Student Loan Payments.\nLoad in the data using the following script.\n\n\nlibrary(tidymodels)\nloans <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-11-26/loans.csv\") %>%\n  select(-agency_name, -added) %>%\n  drop_na()\n\n\n\nUse the prcomp() function to perform PCA on the loans data set. Set scale. = TRUE to perform scaling. What results are contained in this object? (hint: use the names() function)\nCalculate the amount of variance explained by each principal component. (hint: look at ?broom:::tidy.prcomp)\nUse the tidy() function to extract the the loadings. Which variable contributed most to the first principle component? Second Component?\nUse the augment() function to get back the transformation and create a scatter plot of any two components of your choice.\nExercise 4\nIn this exercise, you are tasked to predict the weight of an animal in a zoo, based on which words are used to describe it. The animals data set can be downloaded here.\nThis data set contains 1001 variables. The first variable weight is the natural log of the mean weight of the animal. The remaining variables are named tf_* which shows how many times the word * appears in the description of the animal.\nUse {tidymodels} to set up a workflow to train a PC regression. We can do this by specifying a linear regression model, and create a preprocessor recipe with {recipes} that applies PCA transformation on the predictors using step_pca(). Use the threshold argument in step_pca() to only keep the principal components that explain 90% of the variance.\nHow well does this model perform on the testing data set?\n\n\n\n",
      "last_modified": "2021-10-17T14:33:16-07:00"
    },
    {
      "path": "assignment-07.html",
      "title": "Assignment 7",
      "author": [],
      "contents": "\nExercise 1 (12 points)\nFor part (a) through (c) indicate which of the statements are correct. Justify your answers.\nThe lasso, relative to least squares, is:\nMore flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\nMore flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\nLess flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\nLess flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\n\nRepeat (a) for ridge regression relative to least squares.\nRepeat (a) for non-linear methods relative to least squares.\nExercise 2 (10 points)\nSuppose we estimate the regression coefficients in a linear regression model by minimizing\n\\[\n\\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum^p_{j=1}\\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\]\nfor a particular value of \\(\\lambda\\). For part (a) through (c) indicate which of the statements are correct. Justify your answers.\nAs we increase \\(\\lambda\\) from 0, the training RSS will:\nIncrease initially, and then eventually start decreasing in an inverted U shape.\nDecrease initially, and then eventually start increasing in a U shape.\nSteadily increase.\nSteadily decrease.\nRemain constant.\n\nRepeat (a) for test RSS.\nRepeat (a) for variance.\nRepeat (a) for squared bias.\nRepeat (a) for the irreducible error.\nExercise 3 (13 points)\nIn this exercise, you are tasked to predict the weight of an animal in a zoo, based on which words are used to describe it. The animals data set can be downloaded here.\nThis data set contains 1001 variables. The first variable weight is the natural log of the mean weight of the animal. The remaining variables are named tf_* which shows how many times the word * appears in the description of the animal.\nFit a lasso regression model to predict weight based on all the other variables.\nUse the tune package to perform hyperparameter tuning to select the best value of \\(\\lambda\\). Use 10 bootstraps as the resamples data set.\nHow well does this model perform on the testing data set?\n\n\n\n",
      "last_modified": "2021-10-17T14:33:16-07:00"
    },
    {
      "path": "assignment-08.html",
      "title": "Assignment 8",
      "author": [],
      "contents": "\nExercise 1\nSuppose we fit a curve with basis functions \\(b_1(X) = X\\), \\(B_2(X) = (X - 1)^2 I(X \\geq 1)\\). Note that \\(I(X \\geq 1)\\) equals 1 for \\(X \\geq 1\\) and 0 otherwise. We fit the linear regression model\n\\[Y = \\beta_0 + \\beta_1b_1(X) + \\beta_2b_2(X) + \\varepsilon\\]\nand obtain the coefficient estimates \\(\\hat \\beta_0 = 1\\), \\(\\hat \\beta_1 = 1\\), \\(\\hat \\beta_2 = -2\\). Sketch the estimated curve between \\(X = -2\\) and \\(X = 2\\). Note the intercepts, slopes and other relevant information. Remember to plot using enough points to adequately see them shape of the curve.\nExercise 2\nSuppose we fit a curve with basis functions \\(b_1(X) = I(0 \\leq X \\leq 2) - (X-1)I(1 \\leq X \\leq 2)\\), \\(B_2(X) = (X - 3) I(3 \\leq X \\leq 4) + I(4 < X \\leq 5)\\). We fit the linear regression model\n\\[Y = \\beta_0 + \\beta_1b_1(X) + \\beta_2b_2(X) + \\varepsilon\\]\nand obtain the coefficient estimates \\(\\hat \\beta_0 = 1\\), \\(\\hat \\beta_1 = 1\\), \\(\\hat \\beta_2 = 3\\). Sketch the estimated curve between \\(X = -2\\) and \\(X = 2\\). Note the intercepts, slopes and other relevant information. Remember to plot using enough points to adequately see them shape of the curve.\nExercise 3\nExplain what happens to the bias/variance trade-off of our model estimates use regression splines.\nExercise 4\nIn this exercise you will analyze the Wage data set. It is found in the ISLR package and can be loaded like so\n\n\nlibrary(ISLR)\ndata(Wage)\n\n\n\nPerform polynomial regression to predict wage using age. Polynomial regression can be performed in tidymodels by using a linear regression model (linear_reg()) with a recipe that performs polynomial expansion (step_poly()). Use cross-validation to select the optimal degree.\nOptional: Make a plot of the resulting polynomial fit to he data.\nFit a step function (using a linear regression model and step_discretize()) and perform cross-validation to choose the optimal number of cuts.\nOptional: Make a plot of the fit.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:17-07:00"
    },
    {
      "path": "assignment-09.html",
      "title": "Assignment 9",
      "author": [],
      "contents": "\nExercise 1\nDraw an example (of your own invention) of a partition of two-dimensional feature space that could result from recursive binary splitting. Your example should contain at least six regions. Draw a decision tree corresponding to this partition. Be sure to label all aspects of your figures, including regions \\(R_1, R_2, ...\\), the cut points \\(t_1, t_2, ...\\), and so forth.\nExercise 2\nProvide a detailed explanation of the algorithm that is used to fit a regression tree.\nExercise 3\nExplain the difference between bagging, boosting, and random forests.\nExercise 4\nYou will be using the lending_club data found modeldata.\n\n\nlibrary(modeldata)\ndata(\"lending_club\")\n\n\n\nThe response is Class and the remaining variables are predictors.\nDo test-training split as usual, and fit a random forest model or boosted tree (your choice) and a linear regression model.\nThe random forest or boosted tree model has a selection of hyper-parameters that you can tune to improve performance. Perform hyperparameter tuning using k-fold cross-validation to find a model with good predictive power. How does this model compare to the linear regression model?\n\n\n\n",
      "last_modified": "2021-10-17T14:33:17-07:00"
    },
    {
      "path": "assignment-10.html",
      "title": "Assignment 10",
      "author": [],
      "contents": "\nExercise 1\nThis problem involves hyperplanes in two dimensions.\nSketch the hyperplane \\(1 + 3X_1 - X_2 = 0\\). Indicate the set of points for which \\(1 + 3X_1 - X_2 > 0\\), as well as the sett of points for which \\(1 + 3X_1 - X_2 < 0\\).\nOn the same plot, sketch the hyperplane \\(-2 + X_1 + 2 X_2 = 0\\). Indicate the set of points for which \\(-2 + X_1 + 2 X_2 > 0\\), as well as the sett of points for which \\(-2 + X_1 + 2 X_2 < 0\\).\nExercise 2\nWe have seen that in \\(p = 2\\) dimensions, a linear decision boundary takes the form \\(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 = 0\\). We now investigate a non-linear decision boundary.\nSketch the curve\n\\[(1 + X_1)^2 + (2 - X_2)^2 = 4\\]\nOn your sketch, indicate the set of points for which\n\\[(1 + X_1)^2 + (2 - X_2)^2 > 4\\]\nas well as the set of points for which\n\\[(1 + X_1)^2 + (2 - X_2)^2 \\leq 4\\]\nSuppose that a classifier assigns an observation to the blue class if\n\\[(1 + X_1)^2 + (2 - X_2)^2 > 4\\]\nand to the red class otherwise. To what class is the observation \\((0, 0)\\) classified? \\((-1, 1)\\)?, \\((2, 2)\\)?, \\((3, 8)\\)?\nExercise 3\nIn this problem, you will use support vector approaches to predict whether a given car gets high or low gas mileage based on the Auto data set from the ISLR package. To the following code to turn the mpg variable to a binary factor that is split by the median value of mpg.\nYou will need to use the\n\n\nlibrary(tidymodels)\nlibrary(ISLR)\n\nAuto <- Auto %>%\n  mutate(mpg = factor(mpg > median(mpg), \n                      levels = c(TRUE, FALSE),\n                      labels = c(\"High\", \"Low\"))) %>%\n  select(-origin, -name)\n\n\n\nUse k-fold cross-validation to fit an SVM with a polynomial radial basis kernel (svm_poly()) to select a good value of cost and degree. Report the different errors associated with different values of this parameter. Comment on your results.\nRepeat with an SVM with a radial basis kernel (svm_rbf()). This model can be tuned over cost and rbf_sigma which is The cost of predicting a sample within or on the wrong side of the margin and The precision parameter for the radial basis function, respectively. Comment on your results.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:20-07:00"
    },
    {
      "path": "assignments.html",
      "title": "Assignments",
      "author": [],
      "contents": "\nWe will 10 weekly assignments, a midterm, and a final project in this class. The placement of these is located in the schedule.\nAssignments are to be turned in on Canvas. Assignments will be available no later than 3 days before class and are due to be turned in on the following Sunday. Specific times can be found on Canvas. The assignment will contain a mix of conceptual, technical, and coding exercises.\nThe midterm will much like the assignments but with a larger focus on a real analysis.\nThe final project will have two parts. The final project will be a full data analysis of data of your choosing. The Project is a chance for you to show that you understand the data and to showcase some of the models and techniques you have learned to use in this class. The project can be done in groups. Both groups and the data you want to use should be approved by email by me. Approval must happen no later than November 14th.\nIdeas for data can be found here:\nhttps://github.com/rfordatascience/tidytuesday\nhttps://www.data-is-plural.com/\nhttps://www.kaggle.com/\n\n\n\n",
      "last_modified": "2021-10-17T14:33:21-07:00"
    },
    {
      "path": "index.html",
      "title": "Statistical Machine Learning",
      "description": "American University 427/627\n",
      "author": [],
      "contents": "\nThis website contains most of the information and material that will be used for one of the sections of the course Statistical Machine Learning at American University.\nThe navigation bar contains information about the syllabus, schedule, readings, labs and assignments.\nLicense\nThis online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license.\nColophon\nThis book was written in RStudio using distill. The complete source is available on GitHub. All packages versions are being handled with renv.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:21-07:00"
    },
    {
      "path": "labs-02.html",
      "title": "Week 2 - Linear regression",
      "author": [],
      "contents": "\nDownload template here\nThe following chunk will set up your document. Run it, then ignore it.\nIf the system prompts you to install a package, or gives you a “package not found” error, simply run install.packages(\"packagename\") once to install it.\nThe data set\nToday we will use the ames data set containing 82 fields were recorded for 2,930 properties in Ames IA. Run the chunk below to load the data, and to check out the first few rows of your data set.\n\n\ndata(\"ames\", package = \"modeldata\")\nhead(ames)\n\n\n\nThe first thing we need to do with any data set is check for missing data, and make sure the variables are the right type:\n\n\names %>% summary()\n\n\n\nLooks good for this data, with one exception:\nThe next thing we should do is visualize our variables to get a feel for what is going on in this data. There is a lot of variables in this data set so we will focus on Sale_Price, Bedroom_AbvGr, and Gr_Liv_Area.\nSale_Price Sale price in USD\nBedroom_AbvGr Bedrooms above grade (does NOT include basement bedrooms)\nGr_Liv_AreaAbove grade (ground) living area square feet\nData dictionary can be found here.\n\n\names %>%\n  ggplot(aes(x = Gr_Liv_Area, y = Sale_Price)) +\n  geom_point()\n\n\n\nWe see that there is some kind of trend between them. Let up filter away the observations with more than 4000 observations.\n\n\names_df <- ames %>%\n  filter(Gr_Liv_Area < 4000)\n\n\n\nplotting again to make sure we are filtering correctly\n\n\names_df %>%\n  ggplot(aes(x = Gr_Liv_Area, y = Sale_Price)) +\n  geom_point()\n\n\n\nLet us see what we get if we include Bedroom_AbvGr in the chart.\n\n\names_df %>%\n  ggplot(aes(x = Gr_Liv_Area, y = Sale_Price, color = Bedroom_AbvGr)) +\n  geom_point()\n\n\n\nFitting a model\nLet’s begin with a simple linear model. We will look at how Gr_Liv_Area affects the Sale_Price.\nOur first step is to establish a which model(s) we want to try on the data.\nFor now, this is just a simple linear model.\nTo establish the model, we need to determine which R package it comes from (the “engine”) and whether we are doing regression or classification.\n(These functions come from the tidymodels package that we loaded in the setup chunk.)\n\n\nlin_reg <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\n\n\nNext, we will fit the model to our data:\n\n\nlin_reg_fit <- lin_reg %>%\n  fit(Sale_Price ~ Gr_Liv_Area, data = ames_df)\n\n\n\nLet’s check out the output of this model fit:\n\n\nlin_reg_fit %>% \n  extract_fit_engine() %>%\n  summary()\n\n\n\nHow do we interpret this?\nThe slope is 116.225. That means that for every square foot of living area the Sale_Price would increase by about 115 USD.\nThe p-value of the model is basically 0. That means this model is very significant, i.e., there is probably a relationship between the variables.\nThe r-squared value is 0.52. This means 52% of the variance in sale prices is explained by the area of the above ground living area.\nResiduals\nNow let’s look the residuals of the model.\nFirst, we can find out what values were predicted by the model:\n\n\names_preds <- lin_reg_fit %>% \n  predict(new_data = ames_df)\names_preds\n\n\n\nThen, we can calculate and visualize the residuals:\n\n\names_resid <- lin_reg_fit %>%\n  augment(new_data = ames) %>%\n  select(Sale_Price, Gr_Liv_Area, .pred, .resid)\n\names_resid %>%\n  ggplot(aes(x = Gr_Liv_Area, y = .resid)) +\n    geom_point()\n\n\n\nDo the residuals seem to represent “random noise”?\nThat is, was our choice of model reasonable?\nWe can also do sepcific predictions to see what\n\n\npredict(lin_reg_fit, tibble(Gr_Liv_Area = 1500))\n\n\n\nMetrics\nIf we are trying to find the “best” model, we should measure how well this one did.\nWe can compute the SSE and MSE “by hand”:\n\n\nsum(ames_resid$.resid^2)\nmean(ames_resid$.resid^2)\n\n\n\nYOUR TURN\nWhat about the bedrooms? What is the patterns with the number of bedrooms? Can you include Bedroom_AbvGr in the model, and interpret the results?\nCreate a new notebook (you can copy this one if you want), and fit a model using both Gr_Liv_Area and Bedroom_AbvGr as predictors. Report your results.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:22-07:00"
    },
    {
      "path": "labs-03.html",
      "title": "Week 3 - Logistic Regression",
      "author": [],
      "contents": "\nDownload template here\nThe following chunk will set up your document. Run it, then ignore it.\n\n\nlibrary(tidymodels)\nlibrary(nycflights13)\n\n\n\nIf the system prompts you to install a package, or gives you a “package not found” error, simply run install.packages(\"packagename\") once to install it.\nThe data set\nWe will be using the flights data set from the nycflights13 package. nycflights13 is an R data package containing all out-bound flights from NYC.\n\n\nglimpse(flights)\n\n\n\nWe will build a classification model that sees if any given flight is delayed or not. Furthermore, let us trim down the number of variables we are working with. Lastly, let us select to only work with flights taken place during the first month.\n\n\nflights1 <- flights %>%\n  mutate(delay = factor(arr_delay > 0, c(TRUE, FALSE),\n                        c(\"Delayed\", \"On time\"))) %>%\n  filter(month == 1, !is.na(delay)) %>%\n  select(delay, hour, minute, dep_delay, carrier, distance)\n\n\n\nnow that we have performed some cleaning, will we proceed to perform a train-test split.\n\n\nflights_split <- initial_split(flights1)\n\nflights_train <- training(flights_split)\nflights_test <- testing(flights_split)\n\n\n\nwe will use the training data set for visual exploratory data analysis to reinforce the idea that we don’t touch the testing data set.\nEDA\nWe can look at many things with this data set. What we want to look at is how any of the variables relate to delay.\n\n\nflights1 %>%\n  count(hour, delay) %>%\n  ggplot(aes(hour, delay, fill = n)) +\n  geom_raster()\n\n\n\nWe see a varied amount of flights throughout the day. This makes sense, no one wants to leave early or late from the airport.\n\n\nflights1 %>%\n  count(minute, delay) %>%\n  ggplot(aes(minute, delay, fill = n)) +\n  geom_raster()\n\n\n\nWe see some strong artifacts in the time of scheduled departure. Most flights leave on a multiple of 5 which we confirm below.\n\n\nflights1 %>%\n  count(minute, sort = TRUE)\n\n\n\nBy combining hour and minute we can look at how much the different flights have departure delays. There are some really long delays in here!\n\n\nflights1 %>%\n  mutate(time = hour * 60 + minute) %>%\n  ggplot(aes(time, dep_delay)) +\n  geom_point()\n\n\n\nIf we color the points by delay we see that it appears that most of the delayed arrivals happen because of a delayed departure.\n\n\nflights1 %>%\n  mutate(time = hour * 60 + minute) %>%\n  ggplot(aes(time, dep_delay, color = delay)) +\n  geom_point(alpha = 0.2)\n\n\n\nModeling\nLet’s begin with a logistic model. We will look at how dep_delay and distance affects delay.\nOur first step is to establish which model(s) we want to try on the data.\nFor now, this is just a logistic model.\nTo establish the model, we need to determine which R package it comes from (the “engine”) and whether we are doing regression or classification.\n(These functions come from the tidymodels package that we loaded in the setup chunk.)\n\n\nlr_spec <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\n\n\n\nNext, we will fit the model to our data:\n\n\nlr_fit <- lr_spec %>%\n  fit(delay ~ dep_delay + distance, data = flights_train)\n\n\n\nLet’s check out the output of this model fit:\n\n\nlr_fit %>% \n  extract_fit_engine() %>%\n  summary()\n\n\n\nthe coefficients are shown as log-odds terms. We could also get this information using tidy()\n\n\ntidy(lr_fit)\n\n\n\nsetting exponentiate = TRUE, gives us the odds instead of log-odds.\n\n\ntidy(lr_fit, exponentiate = TRUE)\n\n\n\nWe can also take a look at how well the model is doing. By using augment() we can generate predictions, and conf_mat() and autoplot() let us create a confusion matrix and visualize it.\n\n\naugment(lr_fit, flights_train) %>%\n  conf_mat(truth = delay, estimate = .pred_class) %>%\n  autoplot(type = \"heatmap\")\n\n\n\n\n\naugment(lr_fit, flights_train) %>%\n  accuracy(truth = delay, estimate = .pred_class)\n\n\n\nYOUR TURN\nExperiment with using some of the other predictors in your model. Are the answers surprising? Evaluate your models with conf_mat() and accuracy().\nOnce you have a model you like, predict on the test data set and calculate the performance metric. Compare to the performance metrics you got for the training data set.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:23-07:00"
    },
    {
      "path": "labs-04.html",
      "title": "Week 4 - LDA, QDA & KNN",
      "author": [],
      "contents": "\nDownload template here\n\n\nlibrary(tidymodels)\nlibrary(nycflights13)\nlibrary(discrim)\n\n\n\nWe will be using the add-on package discrim to access functions to perform discriminant analysis models with parsnip and kknn to perform KNN methods. If the system prompts you to install a package, or gives you a “package not found” error, simply run install.packages(\"packagename\") once to install it.\nThe data set\nWe will be using the same flights data set from the nycflights13 package. nycflights13 is an R data package containing all out-bound flights from NYC.\nWe will build a classification model that sees if any given flight is delayed or not. Furthermore, let us trim down the number of variables we are working with. Lastly, let us select to only work with flights taken place during the first month.\n\n\nflights1 <- flights %>%\n  mutate(delay = factor(arr_delay > 0, c(TRUE, FALSE),\n                        c(\"Delayed\", \"On time\"))) %>%\n  filter(month == 1, !is.na(delay)) %>%\n  select(delay, hour, minute, dep_delay, carrier, distance)\n\n\n\nnow that we have performed some cleaning, will we proceed to perform a train-test split.\n\n\nflights_split <- initial_split(flights1)\n\nflights_train <- training(flights_split)\nflights_test <- testing(flights_split)\n\n\n\nNow would be a good time to do EDA, but we habe already done the EDA for this section of the data last week, so we will jump right\nModeling\nWe will repeat the modeling we did last week but this time use a LDA, QDA and KNN specification.\nThe specification for each of these models can be found here in the following chunk.\n\n\nlda_spec <- discrim_linear() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"MASS\")\n\nqda_spec <- discrim_quad() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"MASS\")\n\nknn_spec <- nearest_neighbor(neighbors = 5) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kknn\")\n\n\n\nNotice how nearest_neighbor() reqruires that we set a number of neighbors.\nYOUR TURN\nFit each of these modes to see how dep_delay and distance affects delay. Evaluate the performance of each if the models using conf_mat() and accuracy().\n\n\n\n",
      "last_modified": "2021-10-17T14:33:24-07:00"
    },
    {
      "path": "labs-05.html",
      "title": "Week 5 - Feature Engineering",
      "author": [],
      "contents": "\nDownload template here\n\n\nlibrary(tidymodels)\nlibrary(nycflights13)\n\n\n\nWe will not be using any new packages to us today. If the system prompts you to install a package, or gives you a “package not found” error, simply run install.packages(\"packagename\") once to install it.\nThe data set\nWe will be using the same flights data set from the nycflights13 package. nycflights13 is an R data package containing all out-bound flights from NYC.\n\n\nflights\n\n\n\nWe start by creating the proper response variable before creating the validation split\n\n\nflights1 <- flights %>%\n  mutate(delay = factor(arr_delay > 0, c(TRUE, FALSE),\n                        c(\"Delayed\", \"On time\"))) %>%\n  select(-c(year, month, day, hour, minute))\n\n\n\nThen we create the test and training data sets.\n\n\nset.seed(1234)\nflights_split <- initial_split(flights1)\n\nflights_train <- training(flights_split)\nflights_test <- testing(flights_split)\n\n\n\nWe will now try some different preprocessing steps with {recipes} to get a hold of it. Every recipe you create starts with a call to recipe.\n\n\nrecipe(delay ~ ., data = flights_train)\n\n\n\nthen we want to add some steps. Before we add steps, let us think about the order they should be computed in. recipes has a vignette on the Ordering of Steps that gives a reasonnable starting point for ordering.\nWe saw that we have some missing data. So let us try to rectify that. We have two different types of data here. numeric and nominal data. Both these types have different ways of handing missing data. We will use a simple mean imputation for numerics and step_unknown() to change NAs in the categorical predictors to \"unknown\"\n\n\nrec1 <- recipe(delay ~ ., data = flights_train) %>%\n  step_impute_mean(all_numeric_predictors()) %>%\n  step_unknown(all_nominal_predictors())\n\n\n\nWe now need to actually calculate the means. We use the prep() function to do that.\n\n\nrec1_prepped <- rec1 %>%\n  prep()\nrec1_prepped\n\n\n\nWe can now apply this recipe to new data. Using the bake() function os what we use here. This function works a lot like predict() in the sense that it takes int a trained/fitted object and is applied to new data.\n\n\nrec1_prepped %>%\n  bake(new_data = flights_train)\n\n\n\nWe could also in this case have set new_data = NULL since we are using the same data as we supplied to recipe()\n\n\nrec1_prepped %>%\n  bake(new_data = NULL)\n\n\n\nWe can also investigate the values that were estimated using the tidy() function. Just using tidy() gives us a list of operations that were performed.\n\n\nrec1_prepped %>%\n  tidy()\n\n\n\nWe can then dig deeper into each operation by specifying the number of the operation. For step_impute_mean() we get the estimated means\n\n\nrec1_prepped %>%\n  tidy(1)\n\n\n\nFor step_unknown() we see which variables were included and what value get got.\n\n\nrec1_prepped %>%\n  tidy(2)\n\n\n\nNext up us dealing with the datetime variable. We will see what happens when we use the features, abbr, label, ordinal and keep_original_cols argument.\n\n\nrec2 <- recipe(~ time_hour, flights_train) %>%\n  step_date(time_hour)\n\nprep(rec2) %>%\n  bake(new_data = NULL)\n\n\n\nYOUR TURN\nCreate a single recipe, containing the above steps. Create dummy variables of all the nominal predictors with step_dummy() and end with a normalization step via step_normalize(). Prep it on the training data set, and bake it on the testing data set.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:25-07:00"
    },
    {
      "path": "labs-07.html",
      "title": "Week 7 - Clustering",
      "author": [],
      "contents": "\nDownload template here\n\n\nlibrary(tidymodels)\nlibrary(beans)\n\n\n\nWe will be using the beans package for the first time today. If the system prompts you to install a package or gives you a “package not found” error, simply run install.packages(\"packagename\") once to install it.\nThe data set\nThis data set contains morphologic image features for a larger number of beans. With more information here. It contains 7 different types of beans\n\n\nbeans %>%\n  count(class)\n\n\n\nWe can also get some information using skimr::skim(). We see no missing data which is nice.\n\n\nskimr::skim(beans)\n\n\n\nWe want to perform K-Means clustering, so we need to work with numeric data. We are hoping that the numeric variables contain some latent groupings/clusters.\n\n\nbeans_scaled <- recipe(class ~ ., data = beans) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  prep() %>%\n  bake(new_data = NULL)\n\nbeans_scaled\n\n\n\nOnce we have a scaled version of our data set we can pass it to kmeans() to perform the clustering. I’m removing the class variable before passing it to kmeans() as it would complain otherwise. I’m setting centers = 2 because I want to get 2 clusters. I also set a seed to ensure I get the same results every time I run this.\n\n\nset.seed(1234)\nbeans_kmeans2 <- beans_scaled %>%\n  select(-class) %>%\n  kmeans(centers = 2)\n\n\n\nWe can now inspect the results:\n\n\nbeans_kmeans2\n\n\n\nAnother way to interact with the fitted model is to use tidiers from the {broom} package. The tidy() function returns the locations of the clusters.\n\n\ntidy(beans_kmeans2)\n\n\n\nglance() gives us some metrics about the final clustering.\n\n\nglance(beans_kmeans2)\n\n\n\nAnd augment() “applies” the clustering to new data.\n\n\naugment(beans_kmeans2, data = beans_scaled) %>%\n  select(.cluster, everything())\n\n\n\nUsing augment() let us do some easy visualizations to see if we are finding some clusters. If we pick 2 predictors, we can create a scatter plot to compare the clusters with the data.\n\n\nbeans_kmeans2 %>%\n  augment(data = beans_scaled) %>%\n  ggplot(aes(area, perimeter)) +\n  geom_point(aes(color = .cluster), alpha = 0.2)\n\n\n\nif we want we can also locate the clusters by using tidy()\n\n\nbeans_kmeans2 %>%\n  augment(data = beans_scaled) %>%\n  ggplot(aes(area, perimeter)) +\n  geom_point(aes(color = .cluster), alpha = 0.2) +\n  geom_text(aes(label = cluster), data = tidy(beans_kmeans2))\n\n\n\nLet us now take a look at where the classes are. They might not line up with these patterns at all!\n\n\nbeans_kmeans2 %>%\n  augment(data = beans_scaled) %>%\n  ggplot(aes(area, perimeter, color = class)) +\n  geom_point(alpha = 0.2)\n\n\n\nBy using faceting we can look at both class and cluster at the same time without spending too much effort\n\n\nbeans_kmeans2 %>%\n  augment(data = beans_scaled) %>%\n  ggplot(aes(area, perimeter, color = class)) +\n  facet_wrap(~.cluster) +\n  geom_point(alpha = 0.2)\n\n\n\nLet us try one more pair of variables.\n\n\nbeans_kmeans2 %>%\n  augment(data = beans_scaled) %>%\n  ggplot(aes(major_axis_length, minor_axis_length, color = .cluster)) +\n  geom_point(alpha = 0.2)\n\n\n\nWe could instead look at the agreement between class and .cluster\n\n\naugment(beans_kmeans2, data = beans_scaled) %>%\n  count(class, .cluster)\n\n\n\nWe can visualize these results too and we see that the model is doing a pretty good job at putting each class into either cluster 1 or 2.\n\n\naugment(beans_kmeans2, data = beans_scaled) %>%\n  count(class, .cluster) %>%\n  ggplot(aes(class, .cluster, fill = n)) +\n  geom_raster()\n\n\n\nYOUR TURN\nYou have some options in what you want to try. You could try to see what happens if you don’t include all the predictors in the data set. How does that change then results? Try fitting the model with a different number of clusters? Analyze the results and see if it is a better fit for your model. Try what happens if you don’t perform scaling on the predictors. Before running this one, try to write down what you would expect to happen, then do the calculations, and write about what you found. You don’t have to do all of this. Pick one or two things.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:26-07:00"
    },
    {
      "path": "labs-09.html",
      "title": "Week 9 - PCA",
      "author": [],
      "contents": "\nDownload template here\n\n\nlibrary(tidymodels)\nlibrary(beans)\n\n\n\nWe will be using the beans package again today. If the system prompts you to install a package or gives you a “package not found” error, simply run install.packages(\"packagename\") once to install it.\nAnalysis\nWe will now see how we can perform Principal Components Analysis in R. We will do two different approaches, using prcomp() directly, and using the {recipes} package. Starting with prcomp(), we have to pass it a matrix or something that can be turned into a matrix. Such as a data.frame with only numeric variables. I’m setting scale. = TRUE to have prcomp() perform the normalization for us.\n\n\nbeans_pca <- beans %>%\n  select(-class) %>%\n  prcomp(scale. = TRUE)\n\n\n\nnow that we have performed PCA, we can explore the resulting object. We will again use tidy(), and augment(). There is no glance() method for prcomp() objects.\nThe tidy() function can be used in a couple of different ways. You can explore these ways by look at ?tidy.prcomp. The default \"scores\" returns the scores, the information about the map from the original space.\n\n\ntidy(beans_pca, matrix = \"scores\")\n\n\n\nThe loadings show us how much each variable influences each PC.\n\n\ntidy(beans_pca, matrix = \"loadings\")\n\n\n\n\n\ntidy(beans_pca, matrix = \"loadings\") %>%\n  filter(PC == 1) %>%\n  ggplot(aes(value, column)) +\n  geom_col()\n\n\n\nLastly, setting matrix = \"eigenvalues\" gives us PC by PC information about the amount of variance explained.\n\n\ntidy(beans_pca, matrix = \"eigenvalues\")\n\n\n\nWe see that 7 PCs are enough to explain over 99% of the variance contained in the 16 original columns. We can also show this visually with ggplot2.\n\n\ntidy(beans_pca, matrix = \"eigenvalues\") %>%\n  ggplot(aes(PC, cumulative)) +\n  geom_point() +\n  geom_line()\n\n\n\nusing augment() applies the transformation to the data that was used to train it. You can also pass in new data to newdata to have the transformation applied to new data.\n\n\naugment(beans_pca)\n\n\n\nWe can plot the first 2 PCs against each other. Remember that the first two PCs account for over 80% of the variance in the data.\n\n\naugment(beans_pca) %>%\n  bind_cols(beans) %>%\n  ggplot(aes(.fittedPC1, .fittedPC2)) +\n  geom_point(alpha = 0.2)\n\n\n\nby binding the original data back on, lets us add color to show things such as the class\n\n\naugment(beans_pca) %>%\n  bind_cols(beans) %>%\n  ggplot(aes(.fittedPC1, .fittedPC2, color = class)) +\n  geom_point(alpha = 0.2)\n\n\n\nor other variables such as the aspect_ratio.\n\n\naugment(beans_pca) %>%\n  bind_cols(beans) %>%\n  ggplot(aes(.fittedPC1, .fittedPC2, color = aspect_ratio)) +\n  geom_point(alpha = 0.2) +\n  scale_color_viridis_c()\n\n\n\nWe can perform these same calculations using the {recipes} package. We can select the number of PCs to keep by using the threshold or num_comp variables.\n\n\nrec_pca <- recipe(class ~ ., data = beans) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_pca(all_numeric_predictors(), threshold = 0.9)\n\n\n\nThis can then be prepped or included in a workflow().\n\n\nrec_pca_prepped <- rec_pca %>%\n  prep() \n\nrec_pca_prepped %>%\n  bake(new_data = NULL)\n\n\n\nWe can also use tidy() on the prepped recipe to get out what we need.\n\n\ntidy(rec_pca_prepped, 2)\n\n\n\nYOUR TURN\nDo some more visualizations of the different PCs. You can also look into how you can use the rec_pca together with an LDA model.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:27-07:00"
    },
    {
      "path": "labs-10.html",
      "title": "Week 10 - Shinkage Methods",
      "author": [],
      "contents": "\nDownload template here\n\n\nlibrary(tidymodels)\nlibrary(nycflights13)\n\n\n\nWe will be using the nycflights13 package again today. If the system prompts you to install a package or gives you a “package not found” error, simply run install.packages(\"packagename\") once to install it.\n\n\nflights1 <- flights %>%\n  mutate(delay = factor(arr_delay > 0, c(TRUE, FALSE),\n                        c(\"Delayed\", \"On time\"))) %>%\n  filter(month == 1, !is.na(delay)) %>%\n  select(delay, hour, minute, dep_delay, carrier, distance, tailnum, origin, dest)\n\nset.seed(1234)\nflights_split <- initial_split(flights1)\n\nflights_train <- training(flights_split)\nflights_test <- testing(flights_split)\n\n\n\nModeling\nWe have already explored this data set quite a bit so we can move on to some modeling. We will be using the logiistic_reg() specification since we are doing binary classification.\n\n\nlasso_spec <- logistic_reg(mixture = 1, penalty = 0) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"glmnet\")\n\n\n\nNext, we need to create a recipe for preprocessing. Here we want to include many variables since we will be doing shrinkage to weed out unimportant predictors.\n\n\nrec_spec <- recipe(delay ~ ., data = flights_train) %>%\n  step_novel(all_nominal_predictors()) %>%\n  step_unknown(all_nominal_predictors()) %>%\n  step_impute_mean(all_numeric_predictors()) %>%\n  step_other(all_nominal_predictors(), threshold = 0.001) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_numeric_predictors())\n\n\n\nWe then can combine this into a workflow. Notice that this is another way we could create a workflow.\n\n\nlasso_wf <- workflow(rec_spec, lasso_spec)\n\n\n\nWe can now fit the model\n\n\nfitted_model <- fit(lasso_wf, data = flights_train)\n\n\n\nand look at the performance of the model.\n\n\nfitted_model %>%\n  augment(new_data = flights_train) %>%\n  conf_mat(truth = delay, estimate = .pred_class)\n\n\n\nBut above we set penaty to a specific value, which isn’t ideal since we don’t know what the best value would be for our case. So this time when we specify the model, we set penalty = tune() to tell the functions that we want to tune the penalty parameter.\n\n\nlasso_spec <- logistic_reg(mixture = 1, penalty = tune()) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"glmnet\")\n\n\n\nand we create a new workflow.\n\n\nlasso_wf <- workflow(rec_spec, lasso_spec)\n\n\n\nTo be able to tune hyperparameters with tune_grid() we need 3 things. A model/workflow specification folds to fit our model across, and the parameter values we want to try. We already have the lasso_spec. We create the cross-validation folds with vfold_cv().\n\n\nset.seed(5678)\nflights_folds <- vfold_cv(flights_train)\n\n\n\nAnd the different hyperparameter values with grid_regular().\n\n\npenalty_grid <- grid_regular(penalty(), levels = 50)\n\n\n\nNow we can run the training loop.\n\n\nlasso_results <- tune_grid(\n  lasso_wf, \n  resamples = flights_folds, \n  grid = penalty_grid\n)\n\n\n\nThe results can then be summarized with either collect_metrics()\n\n\nlasso_results %>%\n  collect_metrics()\n\n\n\nor autoplot(). The regularization in this model doesn’t affect the performance too much.\n\n\nlasso_results %>%\n  autoplot()\n\n\n\nThe best performing hyperparameter pairs can be shown with show_best(). Notice how they are all the same.\n\n\nlasso_results %>%\n  show_best(\"roc_auc\")\n\n\n\nWe can pluck out the best hyperparameters\n\n\nselect_best(lasso_results, \"roc_auc\")\n\n\n\nand use them to update our model\n\n\nlasso_wf_final <- finalize_workflow(\n  lasso_wf, \n  select_best(lasso_results, \"roc_auc\")\n)\n\n\n\nAnd now we can fit out final model on the training data set.\n\n\nlasso_wf_final_fit <- fit(lasso_wf_final, data = flights_train)\n\nlasso_wf_final_fit\n\n\n\nYOUR TURN\nRepeat the above procedure, but with a ridge model instead of lasso model. You get a ridge model by setting mixture = 0. Compare the performance of both models on the training data set, pick a model and use that one to evaluate on the testing data set. Explain how you picked your model.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:28-07:00"
    },
    {
      "path": "labs-11.html",
      "title": "Week 11 - Splines",
      "author": [],
      "contents": "\nDownload template here\n\n\nlibrary(tidymodels)\n\n\n\nWe will just be using the tidymodels today. If the system prompts you to install a package or gives you a “package not found” error, simply run install.packages(\"packagename\") once to install it.\nThe Data\nWe will use the ames data set from the modeldata library. It can be loaded using the following code\n\n\ndata(\"ames\", package = \"modeldata\")\names\n\n\n\nwe will set up the splits right away.\n\n\nset.seed(1234)\names_split <- initial_split(ames)\n\names_train <- training(ames_split)\names_test <- testing(ames_split)\n\n\n\nWe are still trying to predict the sale price but we will be trying some new techniques. If we plot the location of the houses by their Longitude and Latitude we can get an idea of the neighborhoods\n\n\nggplot(ames_train, aes(Longitude, Latitude)) +\n  geom_point(alpha = 0.5)\n\n\n\nWe are actually able to visualize the neighbors directly using Neighborhood.\n\n\nggplot(ames_train, aes(Longitude, Latitude, color = Neighborhood)) +\n  geom_point(alpha = 0.5) +\n  guides(color = \"none\")\n\n\n\nWe can also plot the Sale_Price and we see some localized trends. Note that this doesn’t take into account anything related to the size and features of the houses.\n\n\nggplot(ames_train, aes(Longitude, Latitude, color = Sale_Price)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c()\n\n\n\nWe can also look at the date 1-dimensionally, and we see some non-linear effects happening here.\n\n\nggplot(ames_train, aes(Longitude, Sale_Price)) +\n  geom_point()\n\n\n\nThe Modeling\nWe have been looking at a lot of different methods this week. Many of these things are available in {recipes} steps. We will explore some of those in this lab. Let us start with step_poly(), this will create a polynomial expansion of the variables.\n\n\nrec_poly <- recipe(Sale_Price ~ Longitude + Latitude, data = ames_train) %>%\n  step_poly(Longitude, Latitude)\n\n\n\nWe will then combine it with a linear regression specification, into a workflow.\n\n\nlm_spec <- linear_reg()\n\npoly_wf <- workflow(rec_poly, lm_spec)\n\n\n\nand we fit the model\n\n\npoly_wf_fit <- fit(poly_wf, data = ames_train)\n\n\n\nWe have seen before how we can calculate metrics and other things to validate the model. This time let us do a more visual inspection of the performance. Let us plot the predicted values on the map we created earlier.\n\n\naugment(poly_wf_fit, new_data = ames_train) %>%\n  ggplot(aes(Longitude, Latitude, color = .pred)) +\n  geom_point(alpha = 0.5) +\n  scale_color_viridis_c()\n\n\n\nWhile this is cool, it can make it hard to see where we are doing well and where we aren’t. So we can plot the residuals. By using a diverging color palette can we see where we are doing well and where we aren’t.\n\n\naugment(poly_wf_fit, new_data = ames_train) %>%\n  ggplot(aes(Longitude, Latitude, color = Sale_Price - .pred)) +\n  geom_point(alpha = 0.5) +\n  scale_color_gradient2()\n\n\n\nYour turn\nTry different non-linear transformations. step_bs() to fit a splines, or step_discretize() or step_cut() to fit step functions. All of these have a hyperparameter you could tune. Try tuning one to see if that improves your model over the default settings.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:29-07:00"
    },
    {
      "path": "labs-12.html",
      "title": "Week 12 - Trees",
      "author": [],
      "contents": "\nDownload template here\nWe will be using tidymodels and the beans data set.\n\n\nlibrary(tidymodels)\nlibrary(beans)\n\n\n\nWe will also use a couple of other packages such as rpart.plot, rpart, ranger, and vip. The {beans} need no further introduction. We will split it right away.\n\n\nset.seed(1234)\nbeans_split <- initial_split(beans)\n\nbeans_train <- training(beans_split)\nbeans_test <- testing(beans_split)\n\n\n\nThe models\nWe will start using a decision_tree(). These can be used for both regression and classification so we need to specify it for this model. We will be using the rpart package as the engine.\n\n\ntree_spec <- decision_tree() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"rpart\")\n\n\n\nand then we will fit it right away. We could do some pre-processing, but in this case, it should be needed. Trees can handle the numeric data just fine, even without scaling.\n\n\nbean_tree <- fit(tree_spec, class ~ ., data = beans_train)\nbean_tree\n\n\n\nThe model has been written out in completion.\nWe can use the rpart.plot() function from the rpart.plot package to visualize the tree in its entirety\n\n\nbean_tree %>%\n  extract_fit_engine() %>%\n  rpart.plot::rpart.plot(box.palette = as.list(rainbow(7)))\n\n\n\nWe can also get the confusion matrix\n\n\nbean_tree %>%\n  augment(new_data = beans_train) %>%\n  conf_mat(class, .pred_class) %>%\n  autoplot(type = \"heatmap\")\n\n\n\nand calculate the accuracy.\n\n\nbean_tree %>%\n  augment(new_data = beans_train) %>%\n  accuracy(class, .pred_class)\n\n\n\ndecision trees have a couple of parameters we could change. Below is what happens when we increase the cost complexity (it defaults to 0.05).\n\n\nbean_tree01 <- tree_spec %>%\n  set_args(cost_complexity = 0.1) %>%\n  fit(class ~ ., data = beans_train)\n\n\n\nThis tree is a lot smaller, it even neglects a couple of the classes due to its size.\n\n\nbean_tree01 %>%\n  extract_fit_engine() %>%\n  rpart.plot::rpart.plot(box.palette = as.list(rainbow(7)))\n\n\n\nWhich we can see represented in its confusion matrix. This model is simply unable to predict 2 of our classes.\n\n\nbean_tree01 %>%\n  augment(new_data = beans_train) %>%\n  conf_mat(class, .pred_class) %>%\n  autoplot(type = \"heatmap\")\n\n\n\nIf we wanted some improvements it might be worth it to take a look at a random forest model. We set importance = \"impurity\" so we can extract variable importance after.\n\n\nrf_spec <- rand_forest(trees = 100) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"ranger\", importance = \"impurity\")\n\n\n\nNow we fit like normal\n\n\nrf_fit <- fit(rf_spec, class ~ ., data = beans_train)\n\n\n\nAnd let us look at the performance. It is doing quite well\n\n\nrf_fit %>%\n  augment(new_data = beans_train) %>%\n  accuracy(class, .pred_class)\n\n\n\nAnd the confusion matrix looks good too\n\n\nrf_fit %>%\n  augment(new_data = beans_train) %>%\n  conf_mat(class, .pred_class)\n\n\n\nLastly, we can use the vip() function from the vip package to extract and visualize the variable importance in this model.\n\n\nvip::vip(rf_fit)\n\n\n\nYour turn\nYou have 2 options. A, tune some parameters to see if you can improve the performance. B, give boosted trees a try. Boosted trees will require some hyperparameter tuning as well. Try using grid_latin_hypercube() instead of grid_regular(). The random forest we fit performed really well, so we should properly use some cross-validation to detect if we are overfitting.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:31-07:00"
    },
    {
      "path": "labs-14.html",
      "title": "Week 14 - SVM",
      "author": [],
      "contents": "\nDownload template here\nWe will be using tidymodels and the flights data set from {nycflights13}.\n\n\nlibrary(tidymodels)\nlibrary(nycflights13)\n\n\n\nWe will do the same transformation as we have done before.\n\n\nflights1 <- flights %>%\n  mutate(delay = factor(arr_delay > 0, c(TRUE, FALSE),\n                        c(\"Delayed\", \"On time\"))) %>%\n  filter(month == 1, !is.na(delay)) %>%\n  select(delay, hour, minute, dep_delay, carrier, distance)\n\nset.seed(1234)\nflights_split <- initial_split(flights1)\n\nflights_train <- training(flights_split)\nflights_test <- testing(flights_split)\n\n\n\nThe models\nWe will start using a svm_linear() model . These can be used for both regression and classification so we need to specify it for this model. We will be using the kernlab package as the engine.\n\n\nsvm_lin_spec <- svm_linear() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\")\n\n\n\nand then we will fit it right away. The fitting might take a minute or two but we shouldn’t worry.\n\n\nsvm_lin_fit <- fit(svm_lin_spec, delay ~ ., data = flights_train)\nsvm_lin_fit\n\n\n\nWe can get the confusion matrix\n\n\nsvm_lin_fit %>%\n  augment(new_data = flights_train) %>%\n  conf_mat(delay, .pred_class) %>%\n  autoplot(type = \"heatmap\")\n\n\n\nand calculate the accuracy.\n\n\nbean_tree %>%\n  augment(new_data = beans_train) %>%\n  accuracy(class, .pred_class)\n\n\n\nThey are not doing well.\nLet us try a polynomial SVM model to see if that helps at all.\n\n\nsvm_poly_spec <- svm_poly(degree = 2) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\")\n\nsvm_poly_fit <- fit(svm_poly_spec, delay ~ ., data = flights_train)\nsvm_poly_fit\n\n\n\ncalculating another confusion matrix doesn’t give us much luck.\n\n\nsvm_poly_fit %>%\n  augment(new_data = flights_train) %>%\n  conf_mat(delay, .pred_class) %>%\n  autoplot(type = \"heatmap\")\n\n\n\nYour turn\nBut wait, we didn’t do any preprocessing. Let us do some proper preprocessing to see if we can improve on the model. We also have a cost parameter we could tune. Let us try that as well.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:32-07:00"
    },
    {
      "path": "labs.html",
      "title": "Labs",
      "author": [],
      "contents": "\nEach lecture will be followed by lab time which is sometimes where we will be using R and tidymodels to implement the methods we will have talked about that week.\nLabs are to be turned in on Blackboard. labs will be available no later than 3 days before class and are due to be turned in on the following Sunday. We will be collectively be talking about the labs and come up with some of the answers. To get full credit for labs the document that is turned in should contain not just the right code to solve the problem but also text explaining what has been done.\nDon’t print raw data.frames. If you want to print part, turn it into a tibble\nDon’t print verbose code\nOutput can be excluding by setting echo=FALSE in chunks.\n<div class=\"layout-chunk\" data-layout=\"l-body\">\n\n\n<\/div>\n\n\n\n\n",
      "last_modified": "2021-10-17T14:33:32-07:00"
    },
    {
      "path": "midterm.html",
      "title": "Midterm",
      "author": [],
      "contents": "\nExercise 1 (10 points)\nExplain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction.\nWe collect several different measurements of salmon swimming through a small river. These measurements include; length, width, weight, color, and time of day the measurement was taken. Later down the river data is collected whether the fish survived the journey.\nWe are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 40 similar products that were previously launched. For each product, we have recorded whether it was a big success or failure, price charged for the product, marketing budget, competition price, and ten other variables. This product is scheduled to be rolled out to multiple stores across the midwest.\nYou are a real estate agent and you are looking at predicting the sale price of your upcoming houses based on lot size, house size, number of bathrooms, number of bedrooms, and presence of a pool.\nExercise 2 (10 points)\nWhat are the advantages and disadvantages of a very flexible approach compared to a less flexible approach for regression or classification? If you were you draw the decision boundary for a very flexible classification model how would it look? Under what circumstances might a more flexible approach be preferred to a less flexible approach?\nExercise 3 (10 points)\nExplain the differences between K-nearest neighbor and linear regression for a general regression task. Under what circumstances would a K-nearest neighbor approach perform better than a linear model. The performance here is measured using an appropriate performance metric calculated on the training data set.\nExercise 4 (10 points)\nExplain how the scaling of predictor variables will or won’t be affecting the model fit for K-nearest neighbors, logistic regression, LDA, and QDA.\nExercise 5 (10 points)\nSuppose you are given a data set and told to perform a clustering analysis to determine how many clusters are present. Explain how you would go about doing that.\nExercise 6 (10 points)\nSuppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X. Carefully describe how we might estimate the standard deviation of our prediction.\nExercise 7 (40 points)\nIn this exercise, you will try to fit a classification model. You are given a data set with a response and 10 numeric predictors. You are to fit 2 knn models one with (K = 1) and one with (K = 2), 1 LDA, and one QDA. The data have already been split for you and can be downloaded here vowel_train and vowel_test. Use K-fold cross-validation with K = 10 with accuracy as the performance metric to select 1 of the 4 models. Fit this one model on the training data set, predict on the testing data set, and calculate the testing accuracy and construct a confusion matrix. Comment on your results.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:33-07:00"
    },
    {
      "path": "readings-01.html",
      "title": "Week 1 - Introduction",
      "author": [],
      "contents": "\nMonday\nRead chapter 2 of “An Introduction to Statistical Learning”. This is a big picture chapter that lays the foundation of the rest of the book. It is not expected to have read this before class.\nChapter 1 serves as an introduction to the book, data, and notation. Can be read or skimmed through if you want.\nWe will be using R which can be downloaded here. Additionally, it is also advised to use RStudio which can be downloaded here, but any IDE will work.\nWe will be using the tidymodels ecosystem of packages designed for modeling. If you haven’t already you should install these packages along with tidyverse.\n\n\ninstall.packages(\"tidymodels\")\ninstall.packages(\"tidyverse\")\n\n\n\nThursday\nRead chapter 3 of “An Introduction to Statistical Learning”. This chapter goes over simple linear regression, multiple linear regression, and the considerations used in linear regression. As far as I can tell, you have all taken courses where linear regression has been introduced. It is still worthwhile for you to read this chapter to get familiar with the book, and hopefully, it will be an easy read.\nIf you want more information about the way to fit a model using the tidymodels, read chapter 6 of Tidy Modeling with R, Fitting models with parsnip.\nIf you want more information on the theoretical background you can read sections 3.1 and 3.2 of “The Elements of Statistical Learning”. This is optional reading.\nInstead of reading the lab sections of chapter 3, read this chapter instead which I rewrote to use tidymodels.\nSlides Monday\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download Full Screen\nSlides Thursday\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download Full Screen\n\n\n\n",
      "last_modified": "2021-10-17T14:33:33-07:00"
    },
    {
      "path": "readings-02.html",
      "title": "Week 2 - Linear Regression",
      "author": [],
      "contents": "\nThis week is a half week because of Labor Day. There will be no class Monday, and the Thursday class will be labs about linear regression to accompany the lecture from last week.\nIf you need a reminder this is the reading about linear regression models.\nRead chapter 3 of “An Introduction to Statistical Learning”. This chapter goes over simple linear regression, multiple linear regression, and the considerations used in linear regression. As far as I can tell, you have all taken courses where linear regression has been introduced. It is still worthwhile for you to read this chapter to get familiar with the book, and hopefully, it will be an easy read.\nIf you want more information about the way to fit a model using the tidymodels, read chapter 6 of Tidy Modeling with R, Fitting models with parsnip.\nIf you want more information on the theoretical background you can read sections 3.1 and 3.2 of “The Elements of Statistical Learning”. This is optional reading.\nInstead of reading the lab sections of chapter 3, read this chapter instead which I rewrote to use tidymodels.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:34-07:00"
    },
    {
      "path": "readings-03.html",
      "title": "Week 3 - Logistic Regression",
      "author": [],
      "contents": "\nStart read chapter 4 of “An Introduction to Statistical Learning”, we will color 4.1, 4.2, and 4.3. This chapter covers logistic regression, linear discriminant analysis, quadratic discriminant analysis, and Naive Bayes. We will start by talking about logistic regression this week and talk about linear discriminant analysis and quadratic discriminant analysis next week. Furthermore, this is the first we will be talking about classification methods, take note of the differences between this and regression.\nWe will also talk about the notion of data splitting or “train-test splitting”. Please read the 5th chapter of “Tidy Modeling with R” which can be found here.\nThe reading labs for chapter 4 can be found here\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download Full Screen\n\n\n\n",
      "last_modified": "2021-10-17T14:33:34-07:00"
    },
    {
      "path": "readings-04.html",
      "title": "Week 4 - LDA, QDA, KNN",
      "author": [],
      "contents": "\nThis lecture will be covering Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and K-Nearest Neighbors (KNN). Please finish reading chapter 4 (4.4 and 4.5) in “An Introduction to Statistical Learning”. If you haven’t already please make brush up on section 3.5 where KNN is covered for regression.\nThe reading labs for chapter 4 can be found here\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download Full Screen\n\n\n\n",
      "last_modified": "2021-10-17T14:33:35-07:00"
    },
    {
      "path": "readings-05.html",
      "title": "Week 5 - Feature Engineering",
      "author": [],
      "contents": "\nPlease read Feature engineering with recipes from Tidy Modeling with R.\nrecipes will be introduces as the R package we will use to perform pre-processing.\nAs optional but highly recommended reading is chapter 5, 6, and 8 from Feature Engineering and Selection: A Practical Approach for Predictive Models.\nEncoding Categorical Predictors\nEngineering Numeric Predictors\nHandling Missing Data\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download Full Screen\n\n\n\n",
      "last_modified": "2021-10-17T14:33:35-07:00"
    },
    {
      "path": "readings-06.html",
      "title": "Week 6 - Resampling",
      "author": [],
      "contents": "\nPlease read chapter 5 in “An Introduction to Statistical Learning”, which looks at bootstrapping and cross-validation. In addition will we also talk about data leakage some more. We will also be talking about evaluating model performance so please also read chapter 9 - Judging model effectiveness in “Tidy Modeling with R, which we will use as the introduction for the R package yardstick.\nPlease also read chapter 10 and 11 from Tidy Modeling with R\nResampling for evaluating performance\nComparing models with resampling\nThe accompanied tidymodels labs can be found here.\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download Full Screen\n\n\n\n",
      "last_modified": "2021-10-17T14:33:36-07:00"
    },
    {
      "path": "readings-07.html",
      "title": "Week 7 - Clustering",
      "author": [],
      "contents": "\nPlease read chapter 12 in “An Introduction to Statistical Learning”. This will be the only major jump on the textbook, but it shouldn’t affect reading too much. The chapter covers clustering methods and we will look at “K-Means Clustering and Hierarchical clustering. This will (sadly) be our only peak at unsupervised learning this course. Also read the article K-means clustering with tidy data principles from tidymodels.org which covers how to work with clustering in the tidymodels framework.\nThe accompanied tidymodels labs can be found here.\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download Full Screen\n\n\n\n",
      "last_modified": "2021-10-17T14:33:36-07:00"
    },
    {
      "path": "readings-08.html",
      "title": "Week 8 - Topics in ML",
      "author": [],
      "contents": "\nWe will be working on the midterm this week. I will take the opportunity to talk about topics in Machine Learning on a broader scale then just implementation and statistics.\nA Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle\n11 Short Machine Learning Ethics Videos\nSearch for data!\nIdeas for Final project Data:\nhttps://github.com/rfordatascience/tidytuesday\nhttps://www.data-is-plural.com/\nhttps://www.kaggle.com/\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download Full Screen\n\n\n\n",
      "last_modified": "2021-10-17T14:33:37-07:00"
    },
    {
      "path": "readings-09.html",
      "title": "Week 9 - PCA",
      "author": [],
      "contents": "\nThis week we will take a look at PCA (chapter 10 in “An Introduction to Statistical Learning”) and PCA regression (chapter 6.2).\nThe tidymodels lab sections can be found here and here.\nSlides\nTo be posted!\n\n\n\n",
      "last_modified": "2021-10-17T14:33:37-07:00"
    },
    {
      "path": "readings-10.html",
      "title": "Week 10 - Shrinkage",
      "author": [],
      "contents": "\nPlease finish reading chapter 6 of “An Introduction to Statistical Learning”. The chapter covers Shrinkage methods including ridge and lasso regression.\nThis will be a good time to finally talk about hyper parameter tuning. We will use the tune package and dials package. A great place to start learning about these packages is this post.\nThe tidymodels labs chapter can be found here.\nSlides\nTo be posted!\n\n\n\n",
      "last_modified": "2021-10-17T14:33:37-07:00"
    },
    {
      "path": "readings-11.html",
      "title": "Week 11 - Splines & GAMs",
      "author": [],
      "contents": "\nWe will be talking about splines and Generalized additive models (GAM). Please read chapter 7 in “An Introduction to Statistical Learning”.\nThe tidymodels lab sections can be found here.\nSlides\nTo be posted!\n\n\n\n",
      "last_modified": "2021-10-17T14:33:38-07:00"
    },
    {
      "path": "readings-12.html",
      "title": "Week 12 - Trees",
      "author": [],
      "contents": "\nWe will be talking about Decision trees, bagging, boosting and random forests. Please read chapter 8 in “An Introduction to Statistical Learning”.\nThe tidymodels lab sections can be found here.\nSlides\nTo be posted!\n\n\n\n",
      "last_modified": "2021-10-17T14:33:38-07:00"
    },
    {
      "path": "readings-13.html",
      "title": "Week 13 - SVM",
      "author": [],
      "contents": "\nWe will be talking about Support Vector Machines. Please read chapter 9 in “An Introduction to Statistical Learning”.\nThe tidymodels lab sections can be found here.\nThere will not be any labs this week due to Thanksgiving. The corresponding lab will be done next Monday.\nSlides\nTo be posted!\n\n\n\n",
      "last_modified": "2021-10-17T14:33:39-07:00"
    },
    {
      "path": "readings-14.html",
      "title": "Week 14 - SVM labs",
      "author": [],
      "contents": "\nWe will continue last weeks talk on SVMs and do the corresponding lab.\nThe Thursday class will be dedicated for last minute questions and guidance about the final project.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:39-07:00"
    },
    {
      "path": "readings-15.html",
      "title": "Week 15 - Finals Presentations",
      "author": [],
      "contents": "\nBoth days will be dedicated to presentations of finals.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:40-07:00"
    },
    {
      "path": "readings.html",
      "title": "Readings",
      "author": [],
      "contents": "\nThe readings for this course will be based on the textbooks listed in the syllabus. Readings will primarily be taken from the main textbook, but will sometimes be other material. Additional optional reading will also be listed for each week for the students who want to dig deeper into the content of the week. Information from the optional reading will not be expected knowledge in the homework and the exams and should be seen as a learning opportunity only.\nReading before class is strongly encouraged to allow you to find the areas you have additional questions about and ask them in class.\nSlides will also be posted here for each week.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:40-07:00"
    },
    {
      "path": "reference.html",
      "title": "Reference",
      "author": [],
      "contents": "\nVarious information links will be posted here as needed.\nTimer\n15 minute timer\n\n\n\n",
      "last_modified": "2021-10-17T14:33:41-07:00"
    },
    {
      "path": "schedule.html",
      "title": "Schedule",
      "author": [],
      "contents": "\n\nContents\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\n\n\n\n\n\n\n\nWeek 1\n2021-08-30 to 2021-09-05\nMonday\nTopics: Introductions, What is Statistical Machine Learning?, R, RStudio, Tidymodels\nreadings\nThursday\nTopics: Linear regression, Regression\nWeek 2\n2021-09-06 to 2021-09-12\nTopics: Linear regression, Regression\nreadings\nlabs\nAssignments haven’t been posted yet\nWeek 3\n2021-09-13 to 2021-09-19\nTopics: Logistic Regression, Classification, Train-Test Split\nreadings\nlabs\nAssignments haven’t been posted yet\nWeek 4\n2021-09-20 to 2021-09-26\nTopics: LDA, QDA, K-Nearest Neighbors, Naive Bayes\nreadings\nlabs\nAssignments haven’t been posted yet\nWeek 5\n2021-09-27 to 2021-10-03\nTopics: Feature Engineering, Data Preprocessing\nreadings\nlabs\nNo assignment due to “All American Weekend”\nWeek 6\n2021-10-04 to 2021-10-10\nTopics: Bootstrap, Model Diagnostics, Evaluation Metrics, Cross Validation\nreadings\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 7\n2021-10-11 to 2021-10-17\nTopics: Clustering, K-Means Clustering, Hierarchical Clustering\nreadings\nlabs\nAssignments haven’t been posted yet\nWeek 8\n2021-10-18 to 2021-10-24\nTopics: Topics in ML\nreadings\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 9\n2021-10-25 to 2021-10-31\nTopics: PCA, PCA Regression\nreadings\nlabs\nAssignments haven’t been posted yet\nWeek 10\n2021-11-01 to 2021-11-07\nTopics: Shrinkage Methods, Rigde, Lasso, Hyper Parameter Tuning\nreadings\nlabs\nAssignments haven’t been posted yet\nWeek 11\n2021-11-08 to 2021-11-14\nTopics: Splines, GAM\nreadings\nlabs\nAssignments haven’t been posted yet\nWeek 12\n2021-11-15 to 2021-11-21\nTopics: Decision Trees, Bagging, Boosting, Random Forests\nreadings\nlabs\nAssignments haven’t been posted yet\nWeek 13\n2021-11-22 to 2021-11-28\nTopics: SVM\nreadings\nNo lab due to Thanksgiving\nAssignments haven’t been posted yet\nWeek 14\n2021-11-29 to 2021-12-05\nTopics: SVM Labs\nreadings\nlabs\nAssignments haven’t been posted yet\nWeek 15\n2021-12-06 to 2021-12-12\nTopics: Final Project Presentations\nreadings\nNo labs this week due to Final\nAssignments haven’t been posted yet\n\n\n\n",
      "last_modified": "2021-10-17T14:33:41-07:00"
    },
    {
      "path": "syllabus.html",
      "title": "Syllabus",
      "author": [],
      "contents": "\n\nContents\nInstructor\nPre-requisites\nTextbooks\nMain textbook\nSupplementary books\n\nCourse Plan\nAssignments and grading\nLearning objectives\nOnline help\nSoftware\nLearning during a pandemic\nLauren’s Promise\nSupport Services\nEmergency preparedness\nMathematics & Statistics Tutoring Lab (Don Myers Building)\nAcademic Support and Access Center\nCenter for Diversity & Inclusion (X3651, MGC 201)\nThe Office of Advocacy Services for Interpersonal and Sexual Violence (X7070)\nCounseling Center (x3500)\nReligious Holidays\nAcademic Integrity Code\n\n\nInstructor\nInstructor: Emil Hvitfeldt\nTime: Monday & Thursday 8:20-9:35PM ET - 9:35PM ET time zone (Washington DC time)\nCourse website: https://emilhvitfeldt.github.io/AU-2021fall-627/index.html\nOffice hours: Thursday 9:35PM - 10:35PM & Sunday 3:00PM - 4:00PM\nEmail: emilh@american.edu\nTwiter: @Emil_Hvitfeldt\nE-mail are the best ways to get in contact with me. I will try to respond to all course-related e-mails within 24 hours (really) but also remember that life can be busy and chaotic for everyone (including me!), so if I don’t respond right away, don’t worry!\nPre-requisites\nSTAT 520 “Applied Multivariate Analysis” or STAT 615 “Regression”.\nTextbooks\nMain textbook\nThis book will be required reading and we will aim to cover most of the content.\n“An Introduction to Statistical Learning with Applications in R” by G. James, D. Witten, T. Hastie, and R. Tibshirani; Springer, 2021. ISBN 1071614177 The latest corrected printing is available on James’s page at https://statlearning.com/\nThe lab sections of ISLR have been rewritten to use tidymodels and can be found here.\nSupplementary books\nThese books are by no means necessary to buy or read to complete this course but serve as great stepping stones for deeper study. Some week’s readings will refer to these books for extra readings.\n“The Elements of Statistical Learning: Data Mining, Inference, and Prediction”, by T. Hastie, R. Tibshirani, and J. Friedman, 2nd Edition; Springer, 2009. ISBN 0387848576. Available on Hastie’s page at https://web.stanford.edu/~hastie/Papers/ESLII.pdf [more technical; contains advanced explanations and mathematical proofs].\n“Tidy Modeling with R” by Max Kuhn and Julia Silge. Available online at https://www.tmwr.org/.\nCourse Plan\nIntroduction, motivation, and examples. Understanding large and complex data sets. Statistical learning. First steps in R. [Chap. 1-2].\nReview of regression modeling and analysis; implementation in R. [Chap. 3].\nClassification problems and classification tools. Logistic regression and review of linear discriminant analysis. [Chap. 4]\nResampling methods; bootstrap. [Chap. 5 and lecture notes].\nHigh-dimensional data and shrinkage. Ridge regression. LASSO. Model selection methods and dimension reduction. Principal components. Partial least squares. [Chap. 6]\nNonlinear trends and splines. [Chap. 7; 7.4-7.5]\nRegression trees and decision trees [Chap. 8]\nIntroduction to support vector machines [Chap. 9]\nClustering methods [Chap. 10]\nAdditional topics and applications, if time permits.\nAssignments and grading\nAssignments (30%): During the semester I will assign, collect, and grade assignments. You may receive assistance from other students in the class and me, but your submissions must be composed of your own thoughts, coding, and words. A typical homework will include a few problems to do by hand, to see how things work, and a few realistic problems to do using R. Late submission is accepted at a cost of a 5% deduction for each day, with a maximum deduction of 50%.\nLabs (30%): 30-45 minute labs at the end of each class. Each lab covers the material of the lecture. You will have to submit the solutions of each lab on Blackboard the Sunday after each class.\nmidterm (10%) will much like the assignments but with a larger focus on a real analysis.\nProject (30%) (25% report 5% presentation): Each student will receive or choose a data set with data description, problem formulation, and instructions. Using sound statistical methods, you will do the necessary modeling and data analysis and write a report summarizing your results and answering specific questions of your project. A 10-minute presentation summarizing the report will be given to the class or submitted on Canvas.\n90 – 100 % = A\n87 – 90 % = A-\n83 – 87 % = B+\n80 – 83 % = B\n77 – 80 % = B-\n73 – 77 = C+\n70 – 73 % = C\n60 – 70 % = C-\nPlease schedule a meeting with me if you would like to see or discuss your grade at any point during the semester.\nLearning objectives\nGraduate students (STAT 627)\nStudents will be able to:\nIdentify appropriate statistical learning methods for the given problem involving real data.\nUnderstand the underlying assumptions, verify them, and propose appropriate actions if some assumptions do not hold.\nIdentify other possible problems with messy data, such as multicollinearity, understand their consequences, and propose solutions.\nEvaluate the performance of the chosen regression and classification techniques and compare them.\nApply cross-validation techniques to find the optimal degree of flexibility - the best subset of predictors or the optimal tuning parameters.\nShow, analytically, or empirically, the optimal balance between precision within training data and prediction power.\nIllustrate results with appropriate plots and diagrams.\nUndergraduate students (STAT 427)\nStudents will be able to:\nIdentify appropriate statistical learning methods for the given problem involving real data.\nUnderstand the underlying assumptions, techniques available to verify them, and propose appropriate remedies.\nUse training and testing data to evaluate the performance of the chosen regression and classification techniques and compare them.\nUse available empirical tools to find the optimal balance between precision within training data and prediction power.\nIllustrate results with appropriate plots and diagrams.\nStudents will demonstrate competence in using different statistical learning methods involving large, messy, and multi-dimensional numerical and categorical data. Methods include linear, logistic, and polynomial regression with proper variable selection, linear and quadratic discriminant analysis, K-nearest neighbor classifier, bootstrap, ridge regression, lasso, principal components regression, partial least squares, splines, regression and classification trees, support vector machines, clustering, and related methods. In addition, graduate students (STAT 627) will demonstrate competency in the analytic justification of the chosen methods, tuning of the algorithms, and evaluating their prediction power.\nOnline help\nData science and statistical programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if you’ve been doing this stuff for years!).\nFortunately, there are tons of online resources to help you with this. Two of the most important are StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nIf you use Twitter, post R-related questions, and content with #rstats. The community there is exceptionally generous and helpful.\nSearching for help with R on Google can sometimes be tricky because the program name is, um, a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot,” but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”).\nAdditionally, we have a class chatroom at Slack where anyone in the class can ask questions and anyone can answer. I will monitor Slack regularly and will respond quickly. Ask questions about the readings, assignments, and project. You’ll likely have similar questions as your peers, and you’ll likely be able to answer other people’s questions too.\nSoftware\nWe will be using R and tidymodels in this class. While not required, it is highly recommended that you use an IDE for R, I recommend https://rstudio.com/products/rstudio/.\nLearning during a pandemic\nLife absolutely sucks right now. None of us is really okay. We’re all just pretending.\nYou most likely know people who have lost their jobs, have tested positive for COVID-19, have been hospitalized, or perhaps have even died. You all have increased (or possibly decreased) work responsibilities and increased family care responsibilities—you might be caring for extra people (young and/or old!) right now, and you are likely facing uncertain job prospects (or have been laid off!).\nI’m fully committed to making sure that you learn everything you were hoping to learn from this class! I will make whatever accommodations I can to help you finish your exercises, do well on your projects, and learn and understand the class material. Under ordinary conditions, I am flexible and lenient with grading and course expectations when students face difficult challenges. Under pandemic conditions, that flexibility and leniency are intensified.\nIf you tell me you’re having trouble, I will not judge you or think less of you. I hope you’ll extend me the same grace.\nYou never owe me personal information about your health (mental or physical). You are always welcome to talk to me about things that you’re going through, though. If I can’t help you, I usually know somebody who can.\nIf you need extra help, or if you need more time with something, or if you feel like you’re behind or not understanding everything, do not suffer in silence! Talk to me! I will work with you. I promise.\nLauren’s Promise\nI will listen and believe you if someone is threatening you.\nLauren McCluskey, a 21-year-old honors student-athlete, was murdered on October 22, 2018, by a man she briefly dated on the University of Utah campus. We must all take action to ensure that this never happens again.\nIf you are in immediate danger, call 911 or AU police (202 885-2527).\nIf you are experiencing sexual assault, domestic violence, or stalking, please report it to me and I will connect you to resources or find appropriate contact information for Counseling Center.\nSupport Services\nEmergency preparedness\nIn the event of an emergency, students should refer to the AU Web site http: //www.american.edu/emergency and the AU information line at (202) 885-1100 for general university-wide information. In case of a prolonged closure of the University, I send updates to you by email and will post all announcements on Blackboard.\nMathematics & Statistics Tutoring Lab (Don Myers Building)\nprovides tutoring in Intermediate Mathematics and Statistics. http://www.american.edu/cas/mathstat/tutoring.cfm\nAcademic Support and Access Center\noffers study skills workshops, individual instruction, tutor referrals, Supplemental Instruction, writing support, and technical and practical support and assistance with accommodations for students with physical, medical, or psychological disabilities. Writing support is also available in the Writing Center, Battelle-Tompkins 228.\nCenter for Diversity & Inclusion (X3651, MGC 201)\nis dedicated to enhancing LGBTQ, Multicultural, First Generation, and Women’s experiences on campus and to advance AU’s commitment to respecting & valuing diversity by serving as a resource and liaison to students, staff, and faculty on issues of equity through education, outreach, and advocacy.\nThe Office of Advocacy Services for Interpersonal and Sexual Violence (X7070)\nprovides free and confidential advocacy services for anyone in the campus community who is impacted by sexual violence (sexual assault, dating or domestic violence, and stalking).\nCounseling Center (x3500)\noffers counseling and consultations regarding personal concerns, self-help information, and connections to off-campus mental health resources. Academic Support and Access Center (x3360) offers study skills workshops, individual instruction, tutor referrals, Supplemental Instruction, writing support, and technical and practical support and assistance with accommodations for students with physical, medical, or psychological disabilities.\nReligious Holidays\nStudents may receive accommodation in the course for the observance of a religious and/or cultural holiday. The student should notify the professor as soon as possible should such a need exist. More information about accommodations for religious and/or cultural holidays can be found at www.american.edu/ocl/kay/request-for-religious-accommodation.cfm.\nAcademic Integrity Code\nPlease be sure that you are familiar with AU’s Academic Integrity Code, as I am required to report any cases of academic dishonesty to the dean of CAS. For your review: http://www.american.edu/academics/ integrity/.\n\n\n\n",
      "last_modified": "2021-10-17T14:33:42-07:00"
    }
  ],
  "collections": []
}
