{
  "articles": [
    {
      "path": "assignment-01.html",
      "title": "Assignment 1",
      "author": [],
      "contents": "\nExercise 1 (5 points)\nFor each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.\nThe sample size \\(n\\) is extremely large, and the number of predictors \\(p\\) is small.\nThe number of predictors \\(p\\) is extremely large, and the number of observations \\(n\\) is small.\nThe relationship between the predictors and response is highly non-linear.\nThe variance of the error terms, is extremely high.\nExercise 2 (5 points)\nDescribe the difference between a parametric and non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a noon-parametric approach)? What are its disadvantages?\nExercise 3 (5 points)\nCarefully explain the the difference between the KNN classifier and KNN regression methods. Name a downside when using this model on very large data.\nExercise 4 (5 points)\nSuppose we have a data set with five predictors, \\(X1 =\\) GPA, \\(X2 =\\) extracurricular activities (EA), \\(X3 =\\) Gender (1 for Female and 0 for Male), \\(X4 =\\) Interaction between GPA and EA, and \\(X5 =\\) Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get \\(\\beta_0 = 50\\), \\(\\beta_1 = 20\\), \\(\\beta_2 = 0.07\\), \\(\\beta_3 = 35\\), \\(\\beta_4 = 0.01\\), \\(\\beta_5 = - 10\\).\nWhich answer is correct, and why?\nFor a fixed value of EA and GPA, males earn more on average than females.\nFor a fixed value of EA and GPA, females earn more on average than males.\nFor a fixed value of EA and GPA, males earn more on average than females provided that the GPA is high enough.\nFor a fixed value of EA and GPA, females earn more on average than males provided that the GPA is high enough.\n\nPredict the salary of a female with EA of 110 and a GPA of 4.0.\nTrue or false: Since the coefficient for the GPA/EA interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\nExercise 5 (10 points)\nThis question should be answered using the biomass data set.\n\n\nlibrary(tidymodels)\ndata(\"biomass\")\n\n\n\nFit a multiple regression model to predict HHV using carbon, hydrogen and oxygen.\nProvide an interpretation of each coefficient in the model. Be careful, note the values Cruise is able to take.\nWrite out the model in equation form.\nFor which the predictors can you reject the null hypothesis \\(H_0: \\beta_j = 0\\)?\nOn the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.\nHow well do the models in (a) and (e) fit the data? How big was the effect of removing the predictor?\n\n\n\n",
      "last_modified": "2021-09-30T17:12:55-07:00"
    },
    {
      "path": "assignment-02.html",
      "title": "Assignment 2",
      "author": [],
      "contents": "\nExercise 1 (10 points)\nSuppose we collect data for a group of students in a statistics class with variables \\(X_1\\) = hours studied, \\(X_2\\) = undergrad GPA, and \\(Y\\) = receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat{\\beta}_0=-6\\), \\(\\hat{\\beta}_1=0.05\\), \\(\\hat{\\beta}_2=1\\).\nEstimate the probability that a student who studies for 40 hours and has an undergrad GPA of \\(3.5\\) gets an A in the class.\nHow many hours would that student in part (a) need to study to have a 50% chance of getting an A in the class?\nExercise 2 (5 points)\nSuppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First, we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next, we use 1-nearest neighbors (i.e. \\(K = 1\\)) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?\nExercise 3 (15 points)\nIn this exercise, we will explore a data set about cars called auto which you can find here.\nThe data set contains 1 factor variable and 6 numeric variables. The factor variable mpg has two levels high and low indicating whether the car has a high or low miles per gallon. We will in this exercise investigate if we are able to use a logistic regression classifier to predict if a car has high or low mpg from the other variables.\nRead in the data and create a test-train rsplit object of auto using initial_split(). Use default arguments for initial_split().\nCreate the training and testing data set with training() and testing() respectively.\nFit a logistic regression model using logistic_reg(). Use all the 6 numeric variables as predictors (a formula shorthand is to write mpg ~ . where . means everything. Remember to fit the model only using the training data set.\nInspect the model with summary() and tidy(). Which of the variables are significant?\nPredict values for the training data set and save them as training_pred.\nUse the following code to calculate the training accuracy\n\n\nbind_cols(\n  training_pred,\n  auto_training\n) %>%\n  accuracy(truth = mpg, estimate = .pred_class)\n\n\n\n(auto_training should be renamed to match your training data set if needed.)\nPredict values for the testing data set and use the above code to calculate the testing accuracy. Compare.\n\n\n\n",
      "last_modified": "2021-09-30T17:12:55-07:00"
    },
    {
      "path": "assignment-03.html",
      "title": "Assignment 3",
      "author": [],
      "contents": "\nExercise 1 (10 points)\nYou will in this exercise examine the differences between LDA and QDA.\nIf the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\nIf the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\nIn general, as the sample size \\(n\\) increases, do we expect the test prediction accuracy or QDA relative to LDA to improve, decline, or be unchanged? Why?\nTrue or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\nExecise 3 (20 points)\nThis exercise should be answered using the Weekly data set, which is part of the LSLR package. If you don’t have it installed already you can install it with\n\n\ninstall.packages(\"ISLR\")\n\n\n\nTo load the data set run the following code\n\n\nlibrary(ISLR)\ndata(\"Weekly\")\n\n\n\nThis data is similar in nature to the Smarket data from chapter 4’s lab, it contains 1089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.\nProduce some numerical and graphical summaries of the data. Does there appear to be any patterns?\nUse the whole data set to perform a logistic regression (with logistic_reg()) with Direction as the response and the five lag variables plus Volume as predictors. Use the summary() function to print the results. Do any of the predictors appear to be statistically significant? if so, which ones?\nUse conf_int() and accuracy() from yardstick package to calculate the confusion matrix and the accuracy (overall fraction of correct predictions). Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\nSplit the data into a training and testing data set using the following code\n\n\nweekly_training <- Weekly %>% filter(Year <= 2008)\n weekly_testing <- Weekly %>% filter(Year > 2008)\n \n\n\nNow fit the logistic regression model using the training data, with Lag2 as the only predictor. Compute the confusion matrix and accuracy metric using the testing data set.\nRepeat (e) using LDA.\nRepeat (e) using QDA.\nRepeat (e) using KNN with K = 1.\nWhich of these methods appear to provide the best results on the data?\n(Optional) Experiment with different combinations of predictors for each of the methods. Report the variables, methods, and associated confusion matrix that appears to provide the best results on the held-out data. Note that you can also experiment with different values of K in KNN. (This kind of running many many models and testing on the testing data set many times is not good practice. We will look at ways in later weeks on how we can properly explore multiple models.)\n\n\n\n",
      "last_modified": "2021-09-30T17:12:56-07:00"
    },
    {
      "path": "assignments.html",
      "title": "Assignments",
      "author": [],
      "contents": "\nWe will 10 weekly assignments, a midterm, and a final project in this class. The placement of these is located in the schedule.\nAssignments are to be turned in on Canvas. Assignments will be available no later than 3 days before class and are due to be turned in on the following Sunday. Specific times can be found on Canvas. The assignment will contain a mix of conceptual, technical, and coding exercises.\nThe midterm will much like the assignments but with a larger focus on a real analysis.\nThe final project will have two parts. The final project will be a full data analysis of data of your choosing. The Project is a chance for you to show that you understand the data and to showcase some of the models and techniques you have learned to use in this class. The project can be done in groups. Both groups and the data you want to use should be approved by email by me. Approval must happen no later than November 14th.\nIdeas for data can be found here:\nhttps://github.com/rfordatascience/tidytuesday\nhttps://www.data-is-plural.com/\nhttps://www.kaggle.com/\n\n\n\n",
      "last_modified": "2021-09-30T17:12:56-07:00"
    },
    {
      "path": "index.html",
      "title": "Statistical Machine Learning",
      "description": "American University 427/627\n",
      "author": [],
      "contents": "\nThis website contains most of the information and material that will be used for one of the sections of the course Statistical Machine Learning at American University.\nThe navigation bar contains information about the syllabus, schedule, readings, labs and assignments.\nLicense\nThis online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license.\nColophon\nThis book was written in RStudio using distill. The complete source is available on GitHub. All packages versions are being handled with renv.\n\n\n\n",
      "last_modified": "2021-09-30T17:12:57-07:00"
    },
    {
      "path": "labs-02.html",
      "title": "Week 2 - Linear regression",
      "author": [],
      "contents": "\nDownload template here\nThe following chunk will set up your document. Run it, then ignore it.\nIf the system prompts you to install a package, or gives you a “package not found” error, simply run install.packages(\"packagename\") once to install it.\nThe data set\nToday we will use the ames data set containing 82 fields were recorded for 2,930 properties in Ames IA. Run the chunk below to load the data, and to check out the first few rows of your data set.\n\n\ndata(\"ames\", package = \"modeldata\")\nhead(ames)\n\n\n\nThe first thing we need to do with any data set is check for missing data, and make sure the variables are the right type:\n\n\names %>% summary()\n\n\n\nLooks good for this data, with one exception:\nThe next thing we should do is visualize our variables to get a feel for what is going on in this data. There is a lot of variables in this data set so we will focus on Sale_Price, Bedroom_AbvGr, and Gr_Liv_Area.\nSale_Price Sale price in USD\nBedroom_AbvGr Bedrooms above grade (does NOT include basement bedrooms)\nGr_Liv_AreaAbove grade (ground) living area square feet\nData dictionary can be found here.\n\n\names %>%\n  ggplot(aes(x = Gr_Liv_Area, y = Sale_Price)) +\n  geom_point()\n\n\n\nWe see that there is some kind of trend between them. Let up filter away the observations with more than 4000 observations.\n\n\names_df <- ames %>%\n  filter(Gr_Liv_Area < 4000)\n\n\n\nplotting again to make sure we are filtering correctly\n\n\names_df %>%\n  ggplot(aes(x = Gr_Liv_Area, y = Sale_Price)) +\n  geom_point()\n\n\n\nLet us see what we get if we include Bedroom_AbvGr in the chart.\n\n\names_df %>%\n  ggplot(aes(x = Gr_Liv_Area, y = Sale_Price, color = Bedroom_AbvGr)) +\n  geom_point()\n\n\n\nFitting a model\nLet’s begin with a simple linear model. We will look at how Gr_Liv_Area affects the Sale_Price.\nOur first step is to establish a which model(s) we want to try on the data.\nFor now, this is just a simple linear model.\nTo establish the model, we need to determine which R package it comes from (the “engine”) and whether we are doing regression or classification.\n(These functions come from the tidymodels package that we loaded in the setup chunk.)\n\n\nlin_reg <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\n\n\nNext, we will fit the model to our data:\n\n\nlin_reg_fit <- lin_reg %>%\n  fit(Sale_Price ~ Gr_Liv_Area, data = ames_df)\n\n\n\nLet’s check out the output of this model fit:\n\n\nlin_reg_fit %>% \n  extract_fit_engine() %>%\n  summary()\n\n\n\nHow do we interpret this?\nThe slope is 116.225. That means that for every square foot of living area the Sale_Price would increase by about 115 USD.\nThe p-value of the model is basically 0. That means this model is very significant, i.e., there is probably a relationship between the variables.\nThe r-squared value is 0.52. This means 52% of the variance in sale prices is explained by the area of the above ground living area.\nResiduals\nNow let’s look the residuals of the model.\nFirst, we can find out what values were predicted by the model:\n\n\names_preds <- lin_reg_fit %>% \n  predict(new_data = ames_df)\names_preds\n\n\n\nThen, we can calculate and visualize the residuals:\n\n\names_resid <- lin_reg_fit %>%\n  augment(new_data = ames) %>%\n  select(Sale_Price, Gr_Liv_Area, .pred, .resid)\n\names_resid %>%\n  ggplot(aes(x = Gr_Liv_Area, y = .resid)) +\n    geom_point()\n\n\n\nDo the residuals seem to represent “random noise”?\nThat is, was our choice of model reasonable?\nWe can also do sepcific predictions to see what\n\n\npredict(lin_reg_fit, tibble(Gr_Liv_Area = 1500))\n\n\n\nMetrics\nIf we are trying to find the “best” model, we should measure how well this one did.\nWe can compute the SSE and MSE “by hand”:\n\n\nsum(ames_resid$.resid^2)\nmean(ames_resid$.resid^2)\n\n\n\nYOUR TURN\nWhat about the bedrooms? What is the patterns with the number of bedrooms? Can you include Bedroom_AbvGr in the model, and interpret the results?\nCreate a new notebook (you can copy this one if you want), and fit a model using both Gr_Liv_Area and Bedroom_AbvGr as predictors. Report your results.\n\n\n\n",
      "last_modified": "2021-09-30T17:12:57-07:00"
    },
    {
      "path": "labs-03.html",
      "title": "Week 3 - Logistic Regression",
      "author": [],
      "contents": "\nDownload template here\nThe following chunk will set up your document. Run it, then ignore it.\n\n\nlibrary(tidymodels)\nlibrary(nycflights13)\n\n\n\nIf the system prompts you to install a package, or gives you a “package not found” error, simply run install.packages(\"packagename\") once to install it.\nThe data set\nWe will be using the flights data set from the nycflights13 package. nycflights13 is an R data package containing all out-bound flights from NYC.\n\n\nglimpse(flights)\n\n\n\nWe will build a classification model that sees if any given flight is delayed or not. Furthermore, let us trim down the number of variables we are working with. Lastly, let us select to only work with flights taken place during the first month.\n\n\nflights1 <- flights %>%\n  mutate(delay = factor(arr_delay > 0, c(TRUE, FALSE),\n                        c(\"Delayed\", \"On time\"))) %>%\n  filter(month == 1, !is.na(delay)) %>%\n  select(delay, hour, minute, dep_delay, carrier, distance)\n\n\n\nnow that we have performed some cleaning, will we proceed to perform a train-test split.\n\n\nflights_split <- initial_split(flights1)\n\nflights_train <- training(flights_split)\nflights_test <- testing(flights_split)\n\n\n\nwe will use the training data set for visual exploratory data analysis to reinforce the idea that we don’t touch the testing data set.\nEDA\nWe can look at many things with this data set. What we want to look at is how any of the variables relate to delay.\n\n\nflights1 %>%\n  count(hour, delay) %>%\n  ggplot(aes(hour, delay, fill = n)) +\n  geom_raster()\n\n\n\nWe see a varied amount of flights throughout the day. This makes sense, no one wants to leave early or late from the airport.\n\n\nflights1 %>%\n  count(minute, delay) %>%\n  ggplot(aes(minute, delay, fill = n)) +\n  geom_raster()\n\n\n\nWe see some strong artifacts in the time of scheduled departure. Most flights leave on a multiple of 5 which we confirm below.\n\n\nflights1 %>%\n  count(minute, sort = TRUE)\n\n\n\nBy combining hour and minute we can look at how much the different flights have departure delays. There are some really long delays in here!\n\n\nflights1 %>%\n  mutate(time = hour * 60 + minute) %>%\n  ggplot(aes(time, dep_delay)) +\n  geom_point()\n\n\n\nIf we color the points by delay we see that it appears that most of the delayed arrivals happen because of a delayed departure.\n\n\nflights1 %>%\n  mutate(time = hour * 60 + minute) %>%\n  ggplot(aes(time, dep_delay, color = delay)) +\n  geom_point(alpha = 0.2)\n\n\n\nModeling\nLet’s begin with a logistic model. We will look at how dep_delay and distance affects delay.\nOur first step is to establish which model(s) we want to try on the data.\nFor now, this is just a logistic model.\nTo establish the model, we need to determine which R package it comes from (the “engine”) and whether we are doing regression or classification.\n(These functions come from the tidymodels package that we loaded in the setup chunk.)\n\n\nlr_spec <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\n\n\n\nNext, we will fit the model to our data:\n\n\nlr_fit <- lr_spec %>%\n  fit(delay ~ dep_delay + distance, data = flights_train)\n\n\n\nLet’s check out the output of this model fit:\n\n\nlr_fit %>% \n  extract_fit_engine() %>%\n  summary()\n\n\n\nthe coefficients are shown as log-odds terms. We could also get this information using tidy()\n\n\ntidy(lr_fit)\n\n\n\nsetting exponentiate = TRUE, gives us the odds instead of log-odds.\n\n\ntidy(lr_fit, exponentiate = TRUE)\n\n\n\nWe can also take a look at how well the model is doing. By using augment() we can generate predictions, and conf_mat() and autoplot() let us create a confusion matrix and visualize it.\n\n\naugment(lr_fit, flights_train) %>%\n  conf_mat(truth = delay, estimate = .pred_class) %>%\n  autoplot(type = \"heatmap\")\n\n\n\n\n\naugment(lr_fit, flights_train) %>%\n  accuracy(truth = delay, estimate = .pred_class)\n\n\n\nYOUR TURN\nExperiment with using some of the other predictors in your model. Are the answers surprising? Evaluate your models with conf_mat() and accuracy().\nOnce you have a model you like, predict on the test data set and calculate the performance metric. Compare to the performance metrics you got for the training data set.\n\n\n\n",
      "last_modified": "2021-09-30T17:12:58-07:00"
    },
    {
      "path": "labs-04.html",
      "title": "Week 4 - LDA, QDA & KNN",
      "author": [],
      "contents": "\nDownload template here\n\n\nlibrary(tidymodels)\nlibrary(nycflights13)\nlibrary(discrim)\n\n\n\nWe will be using the add-on package discrim to access functions to perform discriminant analysis models with parsnip and kknn to perform KNN methods. If the system prompts you to install a package, or gives you a “package not found” error, simply run install.packages(\"packagename\") once to install it.\nThe data set\nWe will be using the same flights data set from the nycflights13 package. nycflights13 is an R data package containing all out-bound flights from NYC.\nWe will build a classification model that sees if any given flight is delayed or not. Furthermore, let us trim down the number of variables we are working with. Lastly, let us select to only work with flights taken place during the first month.\n\n\nflights1 <- flights %>%\n  mutate(delay = factor(arr_delay > 0, c(TRUE, FALSE),\n                        c(\"Delayed\", \"On time\"))) %>%\n  filter(month == 1, !is.na(delay)) %>%\n  select(delay, hour, minute, dep_delay, carrier, distance)\n\n\n\nnow that we have performed some cleaning, will we proceed to perform a train-test split.\n\n\nflights_split <- initial_split(flights1)\n\nflights_train <- training(flights_split)\nflights_test <- testing(flights_split)\n\n\n\nNow would be a good time to do EDA, but we habe already done the EDA for this section of the data last week, so we will jump right\nModeling\nWe will repeat the modeling we did last week but this time use a LDA, QDA and KNN specification.\nThe specification for each of these models can be found here in the following chunk.\n\n\nlda_spec <- discrim_linear() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"MASS\")\n\nqda_spec <- discrim_quad() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"MASS\")\n\nknn_spec <- nearest_neighbor(neighbors = 5) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kknn\")\n\n\n\nNotice how nearest_neighbor() reqruires that we set a number of neighbors.\nYOUR TURN\nFit each of these modes to see how dep_delay and distance affects delay. Evaluate the performance of each if the models using conf_mat() and accuracy().\n\n\n\n",
      "last_modified": "2021-09-30T17:12:59-07:00"
    },
    {
      "path": "labs-05.html",
      "title": "Week 5 - Feature Engineering",
      "author": [],
      "contents": "\nDownload template here\n\n\nlibrary(tidymodels)\nlibrary(nycflights13)\n\n\n\nWe will not be using any new packages to us today. If the system prompts you to install a package, or gives you a “package not found” error, simply run install.packages(\"packagename\") once to install it.\nThe data set\nWe will be using the same flights data set from the nycflights13 package. nycflights13 is an R data package containing all out-bound flights from NYC.\n\n\nflights\n\n\n\nWe start by creating the proper response variable before creating the validation split\n\n\nflights1 <- flights %>%\n  mutate(delay = factor(arr_delay > 0, c(TRUE, FALSE),\n                        c(\"Delayed\", \"On time\"))) %>%\n  select(-c(year, month, day, hour, minute))\n\n\n\nThen we create the test and training data sets.\n\n\nset.seed(1234)\nflights_split <- initial_split(flights1)\n\nflights_train <- training(flights_split)\nflights_test <- testing(flights_split)\n\n\n\nWe will now try some different preprocessing steps with {recipes} to get a hold of it. Every recipe you create starts with a call to recipe.\n\n\nrecipe(delay ~ ., data = flights_train)\n\n\n\nthen we want to add some steps. Before we add steps, let us think about the order they should be computed in. recipes has a vignette on the Ordering of Steps that gives a reasonnable starting point for ordering.\nWe saw that we have some missing data. So let us try to rectify that. We have two different types of data here. numeric and nominal data. Both these types have different ways of handing missing data. We will use a simple mean imputation for numerics and step_unknown() to change NAs in the categorical predictors to \"unknown\"\n\n\nrec1 <- recipe(delay ~ ., data = flights_train) %>%\n  step_impute_mean(all_numeric_predictors()) %>%\n  step_unknown(all_nominal_predictors())\n\n\n\nWe now need to actually calculate the means. We use the prep() function to do that.\n\n\nrec1_prepped <- rec1 %>%\n  prep()\nrec1_prepped\n\n\n\nWe can now apply this recipe to new data. Using the bake() function os what we use here. This function works a lot like predict() in the sense that it takes int a trained/fitted object and is applied to new data.\n\n\nrec1_prepped %>%\n  bake(new_data = flights_train)\n\n\n\nWe could also in this case have set new_data = NULL since we are using the same data as we supplied to recipe()\n\n\nrec1_prepped %>%\n  bake(new_data = NULL)\n\n\n\nWe can also investigate the values that were estimated using the tidy() function. Just using tidy() gives us a list of operations that were performed.\n\n\nrec1_prepped %>%\n  tidy()\n\n\n\nWe can then dig deeper into each operation by specifying the number of the operation. For step_impute_mean() we get the estimated means\n\n\nrec1_prepped %>%\n  tidy(1)\n\n\n\nFor step_unknown() we see which variables were included and what value get got.\n\n\nrec1_prepped %>%\n  tidy(2)\n\n\n\nNext up us dealing with the datetime variable. We will see what happens when we use the features, abbr, label, ordinal and keep_original_cols argument.\n\n\nrec2 <- recipe(~ time_hour, flights_train) %>%\n  step_date(time_hour)\n\nprep(rec2) %>%\n  bake(new_data = NULL)\n\n\n\nYOUR TURN\nCreate a single recipe, containing the above steps. Create dummy variables of all the nominal predictors with step_dummy() and end with a normalization step via step_normalize(). Prep it on the training data set, and bake it on the testing data set.\n\n\n\n",
      "last_modified": "2021-09-30T17:13:00-07:00"
    },
    {
      "path": "labs.html",
      "title": "Labs",
      "author": [],
      "contents": "\nEach lecture will be followed by lab time which is sometimes where we will be using R and tidymodels to implement the methods we will have talked about that week.\nLabs are to be turned in on Blackboard. labs will be available no later than 3 days before class and are due to be turned in on the following Sunday. We will be collectively be talking about the labs and come up with some of the answers. To get full credit for labs the document that is turned in should contain not just the right code to solve the problem but also text explaining what has been done.\nDon’t print raw data.frames. If you want to print part, turn it into a tibble\nDon’t print verbose code\nOutput can be excluding by setting echo=FALSE in chunks.\n<div class=\"layout-chunk\" data-layout=\"l-body\">\n\n\n<\/div>\n\n\n\n\n",
      "last_modified": "2021-09-30T17:13:00-07:00"
    },
    {
      "path": "readings-01.html",
      "title": "Week 1 - Introduction",
      "author": [],
      "contents": "\nMonday\nRead chapter 2 of “An Introduction to Statistical Learning”. This is a big picture chapter that lays the foundation of the rest of the book. It is not expected to have read this before class.\nChapter 1 serves as an introduction to the book, data, and notation. Can be read or skimmed through if you want.\nWe will be using R which can be downloaded here. Additionally, it is also advised to use RStudio which can be downloaded here, but any IDE will work.\nWe will be using the tidymodels ecosystem of packages designed for modeling. If you haven’t already you should install these packages along with tidyverse.\n\n\ninstall.packages(\"tidymodels\")\ninstall.packages(\"tidyverse\")\n\n\n\nThursday\nRead chapter 3 of “An Introduction to Statistical Learning”. This chapter goes over simple linear regression, multiple linear regression, and the considerations used in linear regression. As far as I can tell, you have all taken courses where linear regression has been introduced. It is still worthwhile for you to read this chapter to get familiar with the book, and hopefully, it will be an easy read.\nIf you want more information about the way to fit a model using the tidymodels, read chapter 6 of Tidy Modeling with R, Fitting models with parsnip.\nIf you want more information on the theoretical background you can read sections 3.1 and 3.2 of “The Elements of Statistical Learning”. This is optional reading.\nInstead of reading the lab sections of chapter 3, read this chapter instead which I rewrote to use tidymodels.\nSlides Monday\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download Full Screen\nSlides Thursday\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download Full Screen\n\n\n\n",
      "last_modified": "2021-09-30T17:13:01-07:00"
    },
    {
      "path": "readings-02.html",
      "title": "Week 2 - Linear Regression",
      "author": [],
      "contents": "\nThis week is a half week because of Labor Day. There will be no class Monday, and the Thursday class will be labs about linear regression to accompany the lecture from last week.\nIf you need a reminder this is the reading about linear regression models.\nRead chapter 3 of “An Introduction to Statistical Learning”. This chapter goes over simple linear regression, multiple linear regression, and the considerations used in linear regression. As far as I can tell, you have all taken courses where linear regression has been introduced. It is still worthwhile for you to read this chapter to get familiar with the book, and hopefully, it will be an easy read.\nIf you want more information about the way to fit a model using the tidymodels, read chapter 6 of Tidy Modeling with R, Fitting models with parsnip.\nIf you want more information on the theoretical background you can read sections 3.1 and 3.2 of “The Elements of Statistical Learning”. This is optional reading.\nInstead of reading the lab sections of chapter 3, read this chapter instead which I rewrote to use tidymodels.\n\n\n\n",
      "last_modified": "2021-09-30T17:13:01-07:00"
    },
    {
      "path": "readings-03.html",
      "title": "Week 3 - Logistic Regression",
      "author": [],
      "contents": "\nStart read chapter 4 of “An Introduction to Statistical Learning”, we will color 4.1, 4.2, and 4.3. This chapter covers logistic regression, linear discriminant analysis, quadratic discriminant analysis, and Naive Bayes. We will start by talking about logistic regression this week and talk about linear discriminant analysis and quadratic discriminant analysis next week. Furthermore, this is the first we will be talking about classification methods, take note of the differences between this and regression.\nWe will also talk about the notion of data splitting or “train-test splitting”. Please read the 5th chapter of “Tidy Modeling with R” which can be found here.\nThe reading labs for chapter 4 can be found here\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download Full Screen\n\n\n\n",
      "last_modified": "2021-09-30T17:13:02-07:00"
    },
    {
      "path": "readings-04.html",
      "title": "Week 4 - LDA, QDA, KNN",
      "author": [],
      "contents": "\nThis lecture will be covering Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and K-Nearest Neighbors (KNN). Please finish reading chapter 4 (4.4 and 4.5) in “An Introduction to Statistical Learning”. If you haven’t already please make brush up on section 3.5 where KNN is covered for regression.\nThe reading labs for chapter 4 can be found here\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download Full Screen\n\n\n\n",
      "last_modified": "2021-09-30T17:13:02-07:00"
    },
    {
      "path": "readings-05.html",
      "title": "Week 5 - Feature Engineering",
      "author": [],
      "contents": "\nPlease read Feature engineering with recipes from Tidy Modeling with R.\nrecipes will be introduces as the R package we will use to perform pre-processing.\nAs optional but highly recommended reading is chapter 5, 6, and 8 from Feature Engineering and Selection: A Practical Approach for Predictive Models.\nEncoding Categorical Predictors\nEngineering Numeric Predictors\nHandling Missing Data\nSlides\n\n\n\nfitvids('.shareagain', {players: 'iframe'});\n\nPDF download Full Screen\n\n\n\n",
      "last_modified": "2021-09-30T17:13:03-07:00"
    },
    {
      "path": "readings-06.html",
      "title": "Week 6 - Resampling",
      "author": [],
      "contents": "\nPlease read chapter 5 in “An Introduction to Statistical Learning”, which looks at bootstrapping and cross-validation. In addition will we also talk about data leakage some more. We will also be talking about evaluating model performance so please also read chapter 9 - Judging model effectiveness in “Tidy Modeling with R, which we will use as the introduction for the R package yardstick.\nPlease also read chapter 10 and 11 from Tidy Modeling with R\nResampling for evaluating performance\nComparing models with resampling\nThe accompanied tidymodels labs can be found here.\nSlides\nTo be posted!\n\n\n\n",
      "last_modified": "2021-09-30T17:13:03-07:00"
    },
    {
      "path": "readings-07.html",
      "title": "Week 7 - Clustering",
      "author": [],
      "contents": "\nPlease read chapter 12 in “An Introduction to Statistical Learning”. This will be the only major jump on the textbook, but it shouldn’t affect reading too much. The chapter covers clustering methods and we will look at “K-Means Clustering and Hierarchical clustering. This will (sadly) be our only peak at unsupervised learning this course. Also read the article K-means clustering with tidy data principles from tidymodels.org which covers how to work with clustering in the tidymodels framework.\nThe accompanied tidymodels labs can be found here.\nSlides\nTo be posted!\n\n\n\n",
      "last_modified": "2021-09-30T17:13:03-07:00"
    },
    {
      "path": "readings-08.html",
      "title": "Week 7 - Topics in ML",
      "author": [],
      "contents": "\nWe will be working on the midterm this week. I will take the opportunity to talk about topics in Machine Learning on a broader scale then just implementation and statistics.\nI’ll post more reading soon!\nSlides\nTo be posted!\n\n\n\n",
      "last_modified": "2021-09-30T17:13:04-07:00"
    },
    {
      "path": "readings-09.html",
      "title": "Week 9 - PCA",
      "author": [],
      "contents": "\nThis week we will take a look at PCA (chapter 10 in “An Introduction to Statistical Learning”) and PCA regression (chapter 6.2).\nThe tidymodels lab sections can be found here and here.\nSlides\nTo be posted!\n\n\n\n",
      "last_modified": "2021-09-30T17:13:04-07:00"
    },
    {
      "path": "readings-10.html",
      "title": "Week 10 - Shrinkage",
      "author": [],
      "contents": "\nPlease finish reading chapter 6 of “An Introduction to Statistical Learning”. The chapter covers Shrinkage methods including ridge and lasso regression.\nThis will be a good time to finally talk about hyper parameter tuning. We will use the tune package and dials package. A great place to start learning about these packages is this post.\nThe tidymodels labs chapter can be found here.\nSlides\nTo be posted!\n\n\n\n",
      "last_modified": "2021-09-30T17:13:05-07:00"
    },
    {
      "path": "readings-11.html",
      "title": "Week 11 - Splines & GAMs",
      "author": [],
      "contents": "\nWe will be talking about splines and Generalized additive models (GAM). Please read chapter 7 in “An Introduction to Statistical Learning”.\nThe tidymodels lab sections can be found here.\nSlides\nTo be posted!\n\n\n\n",
      "last_modified": "2021-09-30T17:13:05-07:00"
    },
    {
      "path": "readings-12.html",
      "title": "Week 12 - Trees",
      "author": [],
      "contents": "\nWe will be talking about Decision trees, bagging, boosting and random forests. Please read chapter 8 in “An Introduction to Statistical Learning”.\nThe tidymodels lab sections can be found here.\nSlides\nTo be posted!\n\n\n\n",
      "last_modified": "2021-09-30T17:13:06-07:00"
    },
    {
      "path": "readings-13.html",
      "title": "Week 13 - SVM",
      "author": [],
      "contents": "\nWe will be talking about Support Vector Machines. Please read chapter 9 in “An Introduction to Statistical Learning”.\nThe tidymodels lab sections can be found here.\nThere will not be any labs this week due to Thanksgiving. The corresponding lab will be done next Monday.\nSlides\nTo be posted!\n\n\n\n",
      "last_modified": "2021-09-30T17:13:06-07:00"
    },
    {
      "path": "readings-14.html",
      "title": "Week 14 - SVM labs",
      "author": [],
      "contents": "\nWe will continue last weeks talk on SVMs and do the corresponding lab.\nThe Thursday class will be dedicated for last minute questions and guidance about the final project.\n\n\n\n",
      "last_modified": "2021-09-30T17:13:07-07:00"
    },
    {
      "path": "readings-15.html",
      "title": "Week 15 - Finals Presentations",
      "author": [],
      "contents": "\nBoth days will be dedicated to presentations of finals.\n\n\n\n",
      "last_modified": "2021-09-30T17:13:07-07:00"
    },
    {
      "path": "readings.html",
      "title": "Readings",
      "author": [],
      "contents": "\nThe readings for this course will be based on the textbooks listed in the syllabus. Readings will primarily be taken from the main textbook, but will sometimes be other material. Additional optional reading will also be listed for each week for the students who want to dig deeper into the content of the week. Information from the optional reading will not be expected knowledge in the homework and the exams and should be seen as a learning opportunity only.\nReading before class is strongly encouraged to allow you to find the areas you have additional questions about and ask them in class.\nSlides will also be posted here for each week.\n\n\n\n",
      "last_modified": "2021-09-30T17:13:07-07:00"
    },
    {
      "path": "reference.html",
      "title": "Reference",
      "author": [],
      "contents": "\nVarious information links will be posted here as needed.\nTimer\n15 minute timer\n\n\n\n",
      "last_modified": "2021-09-30T17:13:08-07:00"
    },
    {
      "path": "schedule.html",
      "title": "Schedule",
      "author": [],
      "contents": "\n\nContents\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\n\n\n\n\n\n\n\nWeek 1\n2021-08-30 to 2021-09-05\nMonday\nTopics: Introductions, What is Statistical Machine Learning?, R, RStudio, Tidymodels\nreadings\nThursday\nTopics: Linear regression, Regression\nWeek 2\n2021-09-06 to 2021-09-12\nTopics: Linear regression, Regression\nreadings\nlabs\nAssignments haven’t been posted yet\nWeek 3\n2021-09-13 to 2021-09-19\nTopics: Logistic Regression, Classification, Train-Test Split\nreadings\nlabs\nAssignments haven’t been posted yet\nWeek 4\n2021-09-20 to 2021-09-26\nTopics: LDA, QDA, K-Nearest Neighbors, Naive Bayes\nreadings\nlabs\nAssignments haven’t been posted yet\nWeek 5\n2021-09-27 to 2021-10-03\nTopics: Feature Engineering, Data Preprocessing\nreadings\nlabs\nNo assignment due to “All American Weekend”\nWeek 6\n2021-10-04 to 2021-10-10\nTopics: Bootstrap, Model Diagnostics, Evaluation Metrics, Cross Validation\nreadings\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 7\n2021-10-11 to 2021-10-17\nTopics: Clustering, K-Means Clustering, Hierarchical Clustering\nreadings\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 8\n2021-10-18 to 2021-10-24\nTopics: Topics in ML\nreadings\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 9\n2021-10-25 to 2021-10-31\nTopics: PCA, PCA Regression\nreadings\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 10\n2021-11-01 to 2021-11-07\nTopics: Shrinkage Methods, Rigde, Lasso, Hyper Parameter Tuning\nreadings\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 11\n2021-11-08 to 2021-11-14\nTopics: Splines, GAM\nreadings\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 12\n2021-11-15 to 2021-11-21\nTopics: Decision Trees, Bagging, Boosting, Random Forests\nreadings\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 13\n2021-11-22 to 2021-11-28\nTopics: SVM\nreadings\nNo lab due to Thanksgiving\nAssignments haven’t been posted yet\nWeek 14\n2021-11-29 to 2021-12-05\nTopics: SVM Labs\nreadings\nLabs haven’t been posted yet\nAssignments haven’t been posted yet\nWeek 15\n2021-12-06 to 2021-12-12\nTopics: Final Project Presentations\nreadings\nNo labs this week due to Final\nAssignments haven’t been posted yet\n\n\n\n",
      "last_modified": "2021-09-30T17:13:09-07:00"
    },
    {
      "path": "syllabus.html",
      "title": "Syllabus",
      "author": [],
      "contents": "\n\nContents\nInstructor\nPre-requisites\nTextbooks\nMain textbook\nSupplementary books\n\nCourse Plan\nAssignments and grading\nLearning objectives\nOnline help\nSoftware\nLearning during a pandemic\nLauren’s Promise\nSupport Services\nEmergency preparedness\nMathematics & Statistics Tutoring Lab (Don Myers Building)\nAcademic Support and Access Center\nCenter for Diversity & Inclusion (X3651, MGC 201)\nThe Office of Advocacy Services for Interpersonal and Sexual Violence (X7070)\nCounseling Center (x3500)\nReligious Holidays\nAcademic Integrity Code\n\n\nInstructor\nInstructor: Emil Hvitfeldt\nTime: Monday & Thursday 8:20-9:35PM ET - 9:35PM ET time zone (Washington DC time)\nCourse website: https://emilhvitfeldt.github.io/AU-2021fall-627/index.html\nOffice hours: Thursday 9:35PM - 10:35PM & Sunday 3:00PM - 4:00PM\nEmail: emilh@american.edu\nTwiter: @Emil_Hvitfeldt\nE-mail are the best ways to get in contact with me. I will try to respond to all course-related e-mails within 24 hours (really) but also remember that life can be busy and chaotic for everyone (including me!), so if I don’t respond right away, don’t worry!\nPre-requisites\nSTAT 520 “Applied Multivariate Analysis” or STAT 615 “Regression”.\nTextbooks\nMain textbook\nThis book will be required reading and we will aim to cover most of the content.\n“An Introduction to Statistical Learning with Applications in R” by G. James, D. Witten, T. Hastie, and R. Tibshirani; Springer, 2021. ISBN 1071614177 The latest corrected printing is available on James’s page at https://statlearning.com/\nThe lab sections of ISLR have been rewritten to use tidymodels and can be found here.\nSupplementary books\nThese books are by no means necessary to buy or read to complete this course but serve as great stepping stones for deeper study. Some week’s readings will refer to these books for extra readings.\n“The Elements of Statistical Learning: Data Mining, Inference, and Prediction”, by T. Hastie, R. Tibshirani, and J. Friedman, 2nd Edition; Springer, 2009. ISBN 0387848576. Available on Hastie’s page at https://web.stanford.edu/~hastie/Papers/ESLII.pdf [more technical; contains advanced explanations and mathematical proofs].\n“Tidy Modeling with R” by Max Kuhn and Julia Silge. Available online at https://www.tmwr.org/.\nCourse Plan\nIntroduction, motivation, and examples. Understanding large and complex data sets. Statistical learning. First steps in R. [Chap. 1-2].\nReview of regression modeling and analysis; implementation in R. [Chap. 3].\nClassification problems and classification tools. Logistic regression and review of linear discriminant analysis. [Chap. 4]\nResampling methods; bootstrap. [Chap. 5 and lecture notes].\nHigh-dimensional data and shrinkage. Ridge regression. LASSO. Model selection methods and dimension reduction. Principal components. Partial least squares. [Chap. 6]\nNonlinear trends and splines. [Chap. 7; 7.4-7.5]\nRegression trees and decision trees [Chap. 8]\nIntroduction to support vector machines [Chap. 9]\nClustering methods [Chap. 10]\nAdditional topics and applications, if time permits.\nAssignments and grading\nAssignments (30%): During the semester I will assign, collect, and grade assignments. You may receive assistance from other students in the class and me, but your submissions must be composed of your own thoughts, coding, and words. A typical homework will include a few problems to do by hand, to see how things work, and a few realistic problems to do using R. Late submission is accepted at a cost of a 5% deduction for each day, with a maximum deduction of 50%.\nLabs (30%): 30-45 minute labs at the end of each class. Each lab covers the material of the lecture. You will have to submit the solutions of each lab on Blackboard the Sunday after each class.\nmidterm (10%) will much like the assignments but with a larger focus on a real analysis.\nProject (30%) (25% report 5% presentation): Each student will receive or choose a data set with data description, problem formulation, and instructions. Using sound statistical methods, you will do the necessary modeling and data analysis and write a report summarizing your results and answering specific questions of your project. A 10-minute presentation summarizing the report will be given to the class or submitted on Canvas.\n90 – 100 % = A\n87 – 90 % = A-\n83 – 87 % = B+\n80 – 83 % = B\n77 – 80 % = B-\n73 – 77 = C+\n70 – 73 % = C\n60 – 70 % = C-\nPlease schedule a meeting with me if you would like to see or discuss your grade at any point during the semester.\nLearning objectives\nGraduate students (STAT 627)\nStudents will be able to:\nIdentify appropriate statistical learning methods for the given problem involving real data.\nUnderstand the underlying assumptions, verify them, and propose appropriate actions if some assumptions do not hold.\nIdentify other possible problems with messy data, such as multicollinearity, understand their consequences, and propose solutions.\nEvaluate the performance of the chosen regression and classification techniques and compare them.\nApply cross-validation techniques to find the optimal degree of flexibility - the best subset of predictors or the optimal tuning parameters.\nShow, analytically, or empirically, the optimal balance between precision within training data and prediction power.\nIllustrate results with appropriate plots and diagrams.\nUndergraduate students (STAT 427)\nStudents will be able to:\nIdentify appropriate statistical learning methods for the given problem involving real data.\nUnderstand the underlying assumptions, techniques available to verify them, and propose appropriate remedies.\nUse training and testing data to evaluate the performance of the chosen regression and classification techniques and compare them.\nUse available empirical tools to find the optimal balance between precision within training data and prediction power.\nIllustrate results with appropriate plots and diagrams.\nStudents will demonstrate competence in using different statistical learning methods involving large, messy, and multi-dimensional numerical and categorical data. Methods include linear, logistic, and polynomial regression with proper variable selection, linear and quadratic discriminant analysis, K-nearest neighbor classifier, bootstrap, ridge regression, lasso, principal components regression, partial least squares, splines, regression and classification trees, support vector machines, clustering, and related methods. In addition, graduate students (STAT 627) will demonstrate competency in the analytic justification of the chosen methods, tuning of the algorithms, and evaluating their prediction power.\nOnline help\nData science and statistical programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if you’ve been doing this stuff for years!).\nFortunately, there are tons of online resources to help you with this. Two of the most important are StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nIf you use Twitter, post R-related questions, and content with #rstats. The community there is exceptionally generous and helpful.\nSearching for help with R on Google can sometimes be tricky because the program name is, um, a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot,” but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”).\nAdditionally, we have a class chatroom at Slack where anyone in the class can ask questions and anyone can answer. I will monitor Slack regularly and will respond quickly. Ask questions about the readings, assignments, and project. You’ll likely have similar questions as your peers, and you’ll likely be able to answer other people’s questions too.\nSoftware\nWe will be using R and tidymodels in this class. While not required, it is highly recommended that you use an IDE for R, I recommend https://rstudio.com/products/rstudio/.\nLearning during a pandemic\nLife absolutely sucks right now. None of us is really okay. We’re all just pretending.\nYou most likely know people who have lost their jobs, have tested positive for COVID-19, have been hospitalized, or perhaps have even died. You all have increased (or possibly decreased) work responsibilities and increased family care responsibilities—you might be caring for extra people (young and/or old!) right now, and you are likely facing uncertain job prospects (or have been laid off!).\nI’m fully committed to making sure that you learn everything you were hoping to learn from this class! I will make whatever accommodations I can to help you finish your exercises, do well on your projects, and learn and understand the class material. Under ordinary conditions, I am flexible and lenient with grading and course expectations when students face difficult challenges. Under pandemic conditions, that flexibility and leniency are intensified.\nIf you tell me you’re having trouble, I will not judge you or think less of you. I hope you’ll extend me the same grace.\nYou never owe me personal information about your health (mental or physical). You are always welcome to talk to me about things that you’re going through, though. If I can’t help you, I usually know somebody who can.\nIf you need extra help, or if you need more time with something, or if you feel like you’re behind or not understanding everything, do not suffer in silence! Talk to me! I will work with you. I promise.\nLauren’s Promise\nI will listen and believe you if someone is threatening you.\nLauren McCluskey, a 21-year-old honors student-athlete, was murdered on October 22, 2018, by a man she briefly dated on the University of Utah campus. We must all take action to ensure that this never happens again.\nIf you are in immediate danger, call 911 or AU police (202 885-2527).\nIf you are experiencing sexual assault, domestic violence, or stalking, please report it to me and I will connect you to resources or find appropriate contact information for Counseling Center.\nSupport Services\nEmergency preparedness\nIn the event of an emergency, students should refer to the AU Web site http: //www.american.edu/emergency and the AU information line at (202) 885-1100 for general university-wide information. In case of a prolonged closure of the University, I send updates to you by email and will post all announcements on Blackboard.\nMathematics & Statistics Tutoring Lab (Don Myers Building)\nprovides tutoring in Intermediate Mathematics and Statistics. http://www.american.edu/cas/mathstat/tutoring.cfm\nAcademic Support and Access Center\noffers study skills workshops, individual instruction, tutor referrals, Supplemental Instruction, writing support, and technical and practical support and assistance with accommodations for students with physical, medical, or psychological disabilities. Writing support is also available in the Writing Center, Battelle-Tompkins 228.\nCenter for Diversity & Inclusion (X3651, MGC 201)\nis dedicated to enhancing LGBTQ, Multicultural, First Generation, and Women’s experiences on campus and to advance AU’s commitment to respecting & valuing diversity by serving as a resource and liaison to students, staff, and faculty on issues of equity through education, outreach, and advocacy.\nThe Office of Advocacy Services for Interpersonal and Sexual Violence (X7070)\nprovides free and confidential advocacy services for anyone in the campus community who is impacted by sexual violence (sexual assault, dating or domestic violence, and stalking).\nCounseling Center (x3500)\noffers counseling and consultations regarding personal concerns, self-help information, and connections to off-campus mental health resources. Academic Support and Access Center (x3360) offers study skills workshops, individual instruction, tutor referrals, Supplemental Instruction, writing support, and technical and practical support and assistance with accommodations for students with physical, medical, or psychological disabilities.\nReligious Holidays\nStudents may receive accommodation in the course for the observance of a religious and/or cultural holiday. The student should notify the professor as soon as possible should such a need exist. More information about accommodations for religious and/or cultural holidays can be found at www.american.edu/ocl/kay/request-for-religious-accommodation.cfm.\nAcademic Integrity Code\nPlease be sure that you are familiar with AU’s Academic Integrity Code, as I am required to report any cases of academic dishonesty to the dean of CAS. For your review: http://www.american.edu/academics/ integrity/.\n\n\n\n",
      "last_modified": "2021-09-30T17:13:09-07:00"
    }
  ],
  "collections": []
}
