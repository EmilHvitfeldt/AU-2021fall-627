<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Shrinkage Methods and Hyperparameter tunning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Emil Hvitfeldt" />
    <meta name="date" content="2021-06-09" />
    <script src="libs/header-attrs-2.6.6/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, title-slide

# Shrinkage Methods and Hyperparameter tunning
## AU STAT627
### Emil Hvitfeldt
### 2021-06-09

---








&lt;div style = "position:fixed; visibility: hidden"&gt;
`$$\require{color}\definecolor{orange}{rgb}{1, 0.603921568627451, 0.301960784313725}$$`
`$$\require{color}\definecolor{blue}{rgb}{0.301960784313725, 0.580392156862745, 1}$$`
`$$\require{color}\definecolor{pink}{rgb}{0.976470588235294, 0.301960784313725, 1}$$`
&lt;/div&gt;

&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  TeX: {
    Macros: {
      orange: ["{\\color{orange}{#1}}", 1],
      blue: ["{\\color{blue}{#1}}", 1],
      pink: ["{\\color{pink}{#1}}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
&lt;/script&gt;

&lt;style&gt;
.orange {color: #FF9A4D;}
.blue {color: #4D94FF;}
.pink {color: #F94DFF;}
&lt;/style&gt;



# Shrinkage Methods

We saw last Monday how PCA can be used as a dimensionality reduction method

This can help us deal with data sets with many variables, high dimensional data

Another way to deal with high dimensional data is to use **feature selection**

We will talk about .blue[shrinkage methods]

---

# Shrinkage Methods

Fit a model with all the predictors + constraints to the coefficient estimates

These constraints will typically try to drag the coefficients towards zero (hence the name)

Shrinkage methods can significantly reduce the variance of the coefficient estimates

---

# Shrinkage Methods

Two best-known techniques

- Ridge regression
- Lasso regression

---

# Ridge Regression

We recall that the least-squares estimator is found by minimizing RSS

`$$\text{RSS} = \sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2$$`

ridge regression builds on this method

---


# Ridge Regression

Introducing a .orange[shrinkage penalty]

`$$\sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 + \orange{\lambda \sum_{j=1}^p \beta_j^2}$$`

where `\(\lambda \geq 0\)` is a **tuning parameter** 

---

# Choice of `\(\lambda\)`

`$$\sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 + \orange{\lambda \sum_{j=1}^p \beta_j^2}$$`

The value of `\(\lambda\)` have a big effect on the parameter estimates

- If `\(\lambda = 0\)` we don't have any penalization and we get the standard OLS estimates
- If `\(\lambda \rightarrow \infty\)` then all the parameters estimates will be shrunk towards zero

Somewhere in the middle is where interesting things start to happen. The different coefficients will be shrunk at different rates

---

# Choice of `\(\lambda\)`

There is no way to select the .blue[best] value of `\(\lambda\)` from the data directly. We will have to try different values out and pick which one performs best

Luckily for this method, The algorithm can fit the model for multiple values of `\(\lambda\)` at once, leaving us with only 1 model fit

We will see later how this is done

---

# Importance of variable scales

`$$\sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 + \orange{\lambda \sum_{j=1}^p \beta_j^2}$$`

In general, for regression models we are not worried about the scale of the variables as the fit will be the same regardless

However, the penalty term here is not scale-invariant

You must scale your variables to have them influence the model evenly

---

# Bias - Variance trade-off

The beauty of this method is that different values of `\(\lambda\)` slides across the bias/variance spectrum

---

# Lasso Regression

One of the main downsides to ridge regression is that the coefficients will not be shrunk to 0 exactly (unless `\(\lambda = \infty\)`)

We technically have to use all the parameters each time we predict and try to explain the model

Lasso regression tries to deal with this issue

---

# Ridge regression

`$$\sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 + \orange{\lambda \sum_{j=1}^p \beta_j^2}$$`

# Lasso Regression

`$$\sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 + \blue{\lambda \sum_{j=1}^p |\beta_j|}$$`

---

# Lasso Regression

Lasso regression allows us to shrink some of the coefficients directly down to 0 depending on the value of `\(\lambda\)`

This allows Lasso to ask as a variable selection method

---

# Fitting a Ridge Regression Model

First, let us select some data. We will be using the `concrete` data set from {modeldata}.

.pull-left[

```r
library(tidymodels)

data("biomass")

biomass_scaled &lt;- biomass %&gt;%
  select(where(is.numeric)) %&gt;%
  scale() %&gt;%
  as_tibble()
```
]

.pull-right[

```r
biomass_scaled
```

```
## # A tibble: 536 x 6
##     carbon hydrogen  oxygen nitrogen  sulfur     HHV
##      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1  0.149    0.152   0.404   -0.561  -0.482   0.227 
##  2  0.118    0.202   0.252   -0.737  -0.482   0.0188
##  3 -0.0466   0.285   0.711   -0.813  -0.441  -0.229 
##  4 -0.314   -0.408  -0.275    1.87   -0.152  -0.268 
##  5 -0.151   -0.0489  0.199   -0.0649 -0.441  -0.188 
##  6 -0.284    0.243   0.151    0.809  -0.276  -0.184 
##  7 -0.110    0.444  -0.0354   1.35   -0.0694 -0.130 
##  8 -0.255    0.202   0.104    0.524  -0.0694 -0.236 
##  9  0.0497   0.0346  0.215   -0.233  -0.482  -0.140 
## 10 -0.117    0.369   0.132    0.103  -0.276  -0.0816
## # … with 526 more rows
```
]

---

# Fitting a Ridge Regression Model

A Ridge regression model specification can be specified using `linear_reg()` with `mixture = 0`


```r
ridge_spec &lt;- linear_reg(mixture = 0, penalty = 0) %&gt;%
  set_engine("glmnet")
```

Then we can fit it just like normal


```r
ridge_fit &lt;- ridge_spec %&gt;%
  fit(HHV ~ ., data = biomass_scaled)
```

---

# Plotting coefficients for Ridge Model


```r
plot(ridge_fit$fit, xvar = "lambda")
```

&lt;img src="index_files/figure-html/unnamed-chunk-6-1.png" width="700px" style="display: block; margin: auto;" /&gt;

---

# Predicting for a Ridge Regression model


```r
predict(ridge_fit, new_data = biomass_scaled)
```

```
## # A tibble: 536 x 1
##      .pred
##      &lt;dbl&gt;
##  1  0.106 
##  2  0.0993
##  3 -0.0543
##  4 -0.337 
##  5 -0.157 
##  6 -0.262 
##  7 -0.0995
##  8 -0.220 
##  9  0.0167
## 10 -0.0916
## # … with 526 more rows
```

---

# Predicting for a Ridge Regression model

Specifying a value lets up predict


```r
predict(ridge_fit, new_data = biomass_scaled, penalty = 100)
```

```
## # A tibble: 536 x 1
##        .pred
##        &lt;dbl&gt;
##  1 -0.000760
##  2 -0.000103
##  3 -0.00406 
##  4 -0.00313 
##  5 -0.00260 
##  6 -0.00515 
##  7 -0.00353 
##  8 -0.00401 
##  9 -0.000851
## 10 -0.00287 
## # … with 526 more rows
```

---

# Predicting for a Ridge Regression model

We could also specify the penalty directly when we fit the model, but there is not as often a usecase for this


```r
ridge_spec &lt;- linear_reg(mixture = 0, penalty = 100) %&gt;%
  set_engine("glmnet")

ridge_fit &lt;- ridge_spec %&gt;%
  fit(HHV ~ ., data = biomass_scaled)

predict(ridge_fit, new_data = biomass_scaled)
```

```
## # A tibble: 536 x 1
##        .pred
##        &lt;dbl&gt;
##  1 -0.000760
##  2 -0.000103
##  3 -0.00406 
##  4 -0.00313 
##  5 -0.00260 
##  6 -0.00515 
##  7 -0.00353 
##  8 -0.00401 
##  9 -0.000851
## 10 -0.00287 
## # … with 526 more rows
```

---

# Fitting a Lasso Regression model

Fitting a lasso model is done the same way a ridge model is fit, instead, we have to set `mixture = 1`


```r
lasso_spec &lt;- linear_reg(mixture = 1, penalty = 0) %&gt;%
  set_engine("glmnet")

lasso_fit &lt;- lasso_spec %&gt;%
  fit(HHV ~ ., data = biomass_scaled)
```

---

# Plotting coefficients for Lasso Model


```r
lasso_fit$fit %&gt;%
  plot(xvar = "lambda")
```

&lt;img src="index_files/figure-html/unnamed-chunk-11-1.png" width="700px" style="display: block; margin: auto;" /&gt;

---

# Ridge VS Lasso

.pull-left[
&lt;img src="index_files/figure-html/unnamed-chunk-12-1.png" width="700px" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="index_files/figure-html/unnamed-chunk-13-1.png" width="700px" style="display: block; margin: auto;" /&gt;
]

---

# Hyperparameter Tuning

Some models have parameters that can not directly be estimated from the data alone. These parameters are found by other means

The "simple" way is to try a lot of different values and pick the one that performs the best

---

# Hyperparameter Tuning

We will use a cross-validation approach to find the best value for the penalty.

we start by splitting up our data and creating some folds.


```r
biomass_split &lt;- initial_split(biomass)

biomass_train &lt;- training(biomass_split)

biomass_folds &lt;- vfold_cv(biomass_train, v = 5)
```


---

# Hyperparameter Tuning

Now we create a new lasso specification, but this time we use `tune()` to denote that we want to tune the `penalty` parameter.


```r
lasso_spec &lt;- linear_reg(mixture = 1, penalty = tune()) %&gt;%
  set_engine("glmnet")
lasso_spec
```

```
## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = tune()
##   mixture = 1
## 
## Computational engine: glmnet
```

---

# Hyperparameter Tuning

We also create a recipe to normalize (scale + center) all the predictors


```r
rec_spec &lt;- recipe(HHV ~ carbon + hydrogen + oxygen + nitrogen + sulfur, 
                   data = biomass) %&gt;%
  step_normalize(all_predictors())
```

And we combine these two into a `workflow`


```r
lasso_wf &lt;- workflow() %&gt;%
  add_model(lasso_spec) %&gt;%
  add_recipe(rec_spec)
```

---

# Hyperparameter Tuning

We also need to specify what values of the hyperparameters we are trying to tune we want to calculate. Since the lasso model can calculate all paths at once let us get back 50 evenly spaced values of `\(\lambda\)`


```r
lambda_grid &lt;- grid_regular(penalty(), levels = 50)
lambda_grid
```

```
## # A tibble: 50 x 1
##     penalty
##       &lt;dbl&gt;
##  1 1   e-10
##  2 1.60e-10
##  3 2.56e-10
##  4 4.09e-10
##  5 6.55e-10
##  6 1.05e- 9
##  7 1.68e- 9
##  8 2.68e- 9
##  9 4.29e- 9
## 10 6.87e- 9
## # … with 40 more rows
```

---

# Hyperparameter Tuning

We combine these things in `tune_grid()` which works much like `fit_resamples()` but takes a `grid` argument as well


```r
tune_rs &lt;- tune_grid(
  object = lasso_wf,
  resamples = biomass_folds,
  grid = lambda_grid
)
```

---

# Hyperparameter Tuning

We can see how each of the values of `\(\lambda\)` is doing with `collect_metrics()`


```r
collect_metrics(tune_rs)
```

```
## # A tibble: 100 x 7
##     penalty .metric .estimator  mean     n std_err .config              
##       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
##  1 1   e-10 rmse    standard   1.53      5  0.258  Preprocessor1_Model01
##  2 1   e-10 rsq     standard   0.833     5  0.0455 Preprocessor1_Model01
##  3 1.60e-10 rmse    standard   1.53      5  0.258  Preprocessor1_Model02
##  4 1.60e-10 rsq     standard   0.833     5  0.0455 Preprocessor1_Model02
##  5 2.56e-10 rmse    standard   1.53      5  0.258  Preprocessor1_Model03
##  6 2.56e-10 rsq     standard   0.833     5  0.0455 Preprocessor1_Model03
##  7 4.09e-10 rmse    standard   1.53      5  0.258  Preprocessor1_Model04
##  8 4.09e-10 rsq     standard   0.833     5  0.0455 Preprocessor1_Model04
##  9 6.55e-10 rmse    standard   1.53      5  0.258  Preprocessor1_Model05
## 10 6.55e-10 rsq     standard   0.833     5  0.0455 Preprocessor1_Model05
## # … with 90 more rows
```

---

# Hyperparameter Tuning

.pull-left[
And there is even a plotting method that can show us how the different values of the hyperparameter are doing
]

.pull-right[

```r
autoplot(tune_rs)
```

&lt;img src="index_files/figure-html/unnamed-chunk-21-1.png" width="700px" style="display: block; margin: auto;" /&gt;
]

---

# Hyperparameter Tuning

Look at the best performing one with `show_best()` and select the best with `select_best()`


```r
tune_rs %&gt;%
  show_best("rmse")
```

```
## # A tibble: 5 x 7
##    penalty .metric .estimator  mean     n std_err .config              
##      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
## 1 1.46e- 2 rmse    standard    1.53     5   0.257 Preprocessor1_Model41
## 2 2.33e- 2 rmse    standard    1.53     5   0.257 Preprocessor1_Model42
## 3 9.10e- 3 rmse    standard    1.53     5   0.258 Preprocessor1_Model40
## 4 1   e-10 rmse    standard    1.53     5   0.258 Preprocessor1_Model01
## 5 1.60e-10 rmse    standard    1.53     5   0.258 Preprocessor1_Model02
```


```r
best_rmse &lt;- tune_rs %&gt;%
  select_best("rmse")
```

---

# Hyperparameter Tuning

Remember how the specification has `penalty = tune()`?


```r
lasso_wf
```

```
## ══ Workflow ════════════════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: linear_reg()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 1 Recipe Step
## 
## • step_normalize()
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = tune()
##   mixture = 1
## 
## Computational engine: glmnet
```

---

# Hyperparameter Tuning

We can update it with `finalize_workflow()`


```r
final_lasso &lt;- finalize_workflow(lasso_wf, best_rmse)
final_lasso
```

```
## ══ Workflow ════════════════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: linear_reg()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 1 Recipe Step
## 
## • step_normalize()
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## Linear Regression Model Specification (regression)
## 
## Main Arguments:
##   penalty = 0.0145634847750124
##   mixture = 1
## 
## Computational engine: glmnet
```

---

# Hyperparameter Tuning

And this finalized specification can now we can fit using the whole training data set.


```r
fitted_lasso &lt;- fit(final_lasso, biomass_train)
```

---

# Hyperparameter Tuning

this fitted model can now be used to predict on the testing data set


```r
biomass_test &lt;- training(biomass_split)

predict(fitted_lasso, biomass_test)
```

```
## # A tibble: 402 x 1
##    .pred
##    &lt;dbl&gt;
##  1  19.1
##  2  28.5
##  3  22.1
##  4  19.1
##  5  19.2
##  6  17.3
##  7  18.3
##  8  19.1
##  9  19.4
## 10  15.9
## # … with 392 more rows
```

---

# Hyperparameter Tuning

&lt;img src="index_files/figure-html/unnamed-chunk-28-1.png" width="700px" style="display: block; margin: auto;" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
